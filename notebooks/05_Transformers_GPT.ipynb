{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cbadenes/curso-pln/blob/main/notebooks/05_Transformers_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5B7RyMVTtO4F"
   },
   "source": [
    "# Generación de Texto usando Transformers\n",
    "\n",
    "Este notebook demuestra cómo usar modelos Transformer para generar texto en español.\n",
    "\n",
    "Utilizaremos el modelo GPT2 específicamente ajustado para español."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsjy7JKGtTuq"
   },
   "source": [
    "##1) Importar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j5y3x4J9tYes"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywi3ROQ2to9I"
   },
   "source": [
    "##2) Cargar el modelo y tokenizador\n",
    "Usamos 'datificate/gpt2-small-spanish', un modelo más ligero entrenado para español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365,
     "referenced_widgets": [
      "bc591cf5ee2f472fb7214cb9fc483300",
      "b6d7da74e71c47158f8185c239bc9346",
      "dda555a34a254b8e88846e59e0f46b8f",
      "f85cacab70954b4bbf2a9ec938aeb891",
      "d63650d2bdad45209256516d8e51d4e2",
      "1e6a62032fb84926b9ed571a3c306c58",
      "ba3e72e053924ef3b87aa71dceeb5f0c",
      "dc82b72ff17a4bffb05a8cd76eaf9621",
      "63f639cd722940eab0568769fbe4f3ad",
      "55e3f370dcc441cfa212dd2ad44cd306",
      "b31745b61c65458cad85179e1accf78f",
      "dcfefe6ab071473585df5e36d0f7edea",
      "f9109a52ae284832bd5e9a35da835571",
      "671402fb5f7e45dfab9ab4fc4a619e97",
      "9b01596298814d0599b4e6c4a3bb27c3",
      "ca8c95a287904e0e9665d554d36ae2c2",
      "df591aecca0245479e101df98b85a8d4",
      "3c9402e7b1a04fb7902e0fd8c66998ba",
      "429e750b8ad8470f88295dd17d176095",
      "2fb9159d59af4fd687bb9e226c8dd758",
      "4c7ad7f721f1456a8a89d0c3bc2917ce",
      "3f96be78c00243b6a2ed81fe02cb13d8",
      "9369700f0a5244cdba71a89f2fe8c783",
      "f47a143a70af4db5be2aac5f7bfd0227",
      "7c7b3e623263406d99bc9ef1ec54ab0e",
      "e6e8fbb1bbbd4c5b88162729b772a74d",
      "84300527977a42639bdd39b404513e53",
      "d3bfa2b6a90048cd9a5852ba03bf8ef0",
      "bfb82d8353a34853bb894f623a01fcb2",
      "e91404278c8e4b25a0aebea9a4ecc1fa",
      "37139aeff96f4fb7812cb8c26d6071ab",
      "dc90b3006d3e46c79716856213e7d7a9",
      "930a0aad62424a0ba2d4e35f3f6d1bb9",
      "d1a7e351f46940d68d171d087b36b0e6",
      "d588aa10a76446318a2041a6025404c0",
      "e8e8d854c6c94eff9e61b49efe868848",
      "a213c66749944b7096434fbfac76f0e3",
      "4d5ce8d9ba934ec3b1bfc00bc98f27c3",
      "ed96f5c8278740c3b06df8d0eac181ca",
      "74259d529c174952be98d1448efcf252",
      "f641ac21d4bd4aa894e07ffbcb5d45ca",
      "041b8be6b46541a19b71841a7025007c",
      "820419dcf8dc47c0b451fc87a75d79ac",
      "60a5300a64d14ce3b8b96d95fb8385fd",
      "cf51aca7e0f14f209d9ccc9742b87f52",
      "be198750d71c4c9f8659cf19f28e99f5",
      "0f5a668a2109417bacb127b12422df79",
      "7a590cb2f9184af0a0c24fd9eee7aa31",
      "e24d46d9828841a480084b714dab0a4e",
      "e79f36629bd9445aac71d77f91f6469e",
      "ed4dd32c19374ec2a29297360dc9c520",
      "b27338a5c2c34607910655a30b7a4a96",
      "03865aa68fa5406a9f2d067be0c6774c",
      "43e48992ef2e41f5b43f68b32fe01df1",
      "841b56528865456a808cb00f3d16408b",
      "6b9a835f543d4119b2fc22376296f969",
      "2e7c636d5f3e43d586bef6a8c3ff168e",
      "d01b431de315455fb4a90197c33567a5",
      "f451537b450f406995816e2a7658b9fc",
      "4abc1bb76efb4e4d9f0d742d34713937",
      "38fd79323df641e1898cdcf326b95a0d",
      "80f0b46ee7f2409baa19623ebb7a3d07",
      "babb953a3a0a431eb97d3c134280ff0e",
      "f9785190f61f47ed8a06d3209b69b35a",
      "160245eda4aa4d3c8fd840619fac2bc1",
      "9b26cce9a5c94f158c8ffcc27b073b04",
      "a39bba0532494567b4d3965371d4a375",
      "51e09444baee4a4fb688da45a85dff41",
      "8a357e92ce844b36bdc7421ddc917d71",
      "ca02a8a502e949c1a1aeafcefae581ce",
      "ed90740258ff4d29974cdfdaa00340c8",
      "db7df4493e434605a1aa96294be1e53b",
      "c562cb4368444bef98b52efa022824b2",
      "cd88d9dc0cfb453eba93194810cd74d7",
      "ca57aeed442b4ed6812d6492d5e0b7cd",
      "c4fd74c7f23349b08ed8ffc32af7b5b3",
      "fc60143aa69b477081a85334737a7931"
     ]
    },
    "id": "GL6sAxvKtr2E",
    "outputId": "a83ea93f-97b9-41d3-d806-ed5a17a3005d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc591cf5ee2f472fb7214cb9fc483300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfefe6ab071473585df5e36d0f7edea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/817 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9369700f0a5244cdba71a89f2fe8c783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/850k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a7e351f46940d68d171d087b36b0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/508k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf51aca7e0f14f209d9ccc9742b87f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9a835f543d4119b2fc22376296f969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39bba0532494567b4d3965371d4a375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"datificate/gpt2-small-spanish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFI-l-pVtueF"
   },
   "source": [
    "##3) Función para generar texto\n",
    "\n",
    "| Parámetro                                 | Descripción                                                                                                                                                             |\n",
    "| ----------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **`inputs[\"input_ids\"]`**                 | Entrada codificada con el tokenizador. Representa el texto base sobre el que se generará la continuación.                                                               |\n",
    "| **`max_length`**                          | Longitud máxima (en tokens) de la secuencia generada, incluyendo los tokens de entrada. Limita el tamaño total del texto generado.                                      |\n",
    "| **`num_return_sequences`**                | Número de secuencias distintas que el modelo debe generar para cada entrada. Por ejemplo, `3` generará tres variantes de texto.                                         |\n",
    "| **`no_repeat_ngram_size=2`**              | Impide que se repitan bigramas (secuencias de 2 palabras seguidas) en el texto generado. Reduce repeticiones molestas.                                                  |\n",
    "| **`do_sample=True`**                      | Habilita la generación con muestreo aleatorio en lugar de usar solo la probabilidad más alta (*greedy decoding*). Necesario para usar `top_k`, `top_p` y `temperature`. |\n",
    "| **`top_k=50`**                            | En cada paso, el modelo considera solo los **50 tokens más probables** (con mayor probabilidad). Reduce el espacio de decisión y evita resultados muy dispersos.        |\n",
    "| **`top_p=0.95`**                          | Usado en *nucleus sampling*: considera los tokens más probables cuya suma acumulada de probabilidad sea ≤ 0.95. Ofrece más variedad que `top_k` solo.                   |\n",
    "| **`temperature=0.7`**                     | Controla la aleatoriedad: valores < 1 hacen la salida más conservadora y enfocada; valores > 1 la hacen más creativa e impredecible.                                    |\n",
    "| **`pad_token_id=tokenizer.eos_token_id`** | Define el token de padding para completar la secuencia si no llega al `max_length`. Aquí se usa el token de fin de secuencia como relleno.                              |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ASxxcHx6tzTZ"
   },
   "outputs": [],
   "source": [
    "def generar_texto(prompt, max_length=100, num_return_sequences=1):\n",
    "    # Tokenizar el texto de entrada\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generar texto\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decodificar y mostrar los resultados\n",
    "    for i, output in enumerate(outputs):\n",
    "        texto_generado = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        print(f\"\\nTexto generado {i+1}:\")\n",
    "        print(texto_generado)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3K7C8DNit1NB"
   },
   "source": [
    "##4) Probar diferentes tipos de generación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "rsqq9FEet5bd",
    "outputId": "0da370a5-5807-4cfb-dca3-b0944076db0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Completando una historia ===\n",
      "\n",
      "Texto generado 1:\n",
      "Había una vez un científico que descubrió su talento y se le dio una beca para estudiar química. El presidente de la Universidad de Harvard, John B. W. Kline, le ofreció una cátedra en química en la Escuela de Medicina Harvard. La segunda mujer fue nombrada directora en 1938. \n",
      "\n",
      "En 1943, fue contratada como asistente del laboratorio de Klicker y, en 1945, ayudó a fundar el Laboratorio Nacional de Química (NSQ) en Washington D. C., en donde se convirtió en una de las primeras mujeres en ser nombradas directora. En 1948, se mudó a los Estados Unidos para trabajar en el Departamento de Ingeniería de Sistemas de Información para el Instituto de Tecnología de Massachusetts. Allí, trabajó en un estudio de campo de energía eléctrica y\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Generando una receta ===\n",
      "\n",
      "Texto generado 1:\n",
      "Para preparar una tortilla española necesitas un gran esfuerzo. En primer lugar, si el animal que se desea se encuentra en el país donde se lleva a cabo el tordo, deberá ser de raza española, o bien de un país extranjero. La torta española es una de las razas de tortas más populares en España.\n",
      "\n",
      "Los tordos españoles, especialmente los machos, son muy apreciados por sus ojos azules y rojizos. Los ojos de los torenos son grandes, oscuros y oscuros. Su carne es gruesa y muy pesada. El tamaño de la cabeza varía de una especie a otra, con más de 6 a 8 kg (3 a 4,5 metros). El peso de esta especie varía en función de su edad y de sus hábitos alimenticios. Algunas especies de tamaño medio y grande son los que tienen más peso en la edad adulta. Las hembras tienen un peso medio de entre 5 y 7 kg. Debido a la gran variedad de colores que presentan en los ojos, el tamaño y\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Generando múltiples variaciones ===\n",
      "\n",
      "Texto generado 1:\n",
      "El futuro de la inteligencia artificial es incierto. La mayoría de los científicos están de acuerdo en que el futuro no puede ser más que un \"deslizamiento\" de las leyes físicas que los humanos pueden construir.\n",
      "\n",
      "La ciencia ficción se ha preocupado por la creación de nuevas razas, y el nuevo planeta natal de un alienígena es una amenaza para la humanidad. Sin embargo, la ciencia moderna ha demostrado ser mucho más poderosa que la tecnología moderna. Al principio, el objetivo de esta amenaza es destruir\n",
      "--------------------------------------------------\n",
      "\n",
      "Texto generado 2:\n",
      "El futuro de la inteligencia artificial en el futuro también puede tener consecuencias en la historia del mundo real.\n",
      "\n",
      "La investigación sobre la evolución del cerebro se remonta a la década de 1940. El primero de estos descubrimientos fue el descubrimiento de una nueva función del cerebelo que permitió a los científicos estudiar el comportamiento de los animales en situaciones donde la precisión de las acciones del hombre era limitada. La investigación comenzó con el desarrollo de un sistema de memoria basado en técnicas de procesamiento de imágenes. Aunque los experimentos comenzaron en\n",
      "--------------------------------------------------\n",
      "\n",
      "Texto generado 3:\n",
      "El futuro de la inteligencia artificial se ve amenazada por una crisis en el orden. El \"Spyros\" es asesinado por los \"Psycho\" en la serie de televisión de ciencia ficción de \"The Flash\". En \"Supergirl\", el héroe de un futuro cercano, Flash, es secuestrado por el \"Moons\" (un grupo de villanos que intentan eliminar a los Flash), y obligado a convertirse en Superman, quien es torturado en un hospital psiquiátrico.\n",
      "\n",
      "La crisis es llevada a\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 1: Completar una historia\n",
    "print(\"=== Completando una historia ===\")\n",
    "prompt_historia = \"Había una vez un científico que descubrió\"\n",
    "generar_texto(prompt_historia, max_length=150)\n",
    "\n",
    "# Ejemplo 2: Generar una receta\n",
    "print(\"\\n=== Generando una receta ===\")\n",
    "prompt_receta = \"Para preparar una tortilla española necesitas\"\n",
    "generar_texto(prompt_receta, max_length=200)\n",
    "\n",
    "# Ejemplo 3: Múltiples variaciones\n",
    "print(\"\\n=== Generando múltiples variaciones ===\")\n",
    "prompt_variaciones = \"El futuro de la inteligencia artificial\"\n",
    "generar_texto(prompt_variaciones, max_length=100, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVR7S2Nft7_M"
   },
   "source": [
    "##5) Funcion Interactiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLTRqaJyuBWC",
    "outputId": "309a6a2a-4377-4b55-f368-8278970fca16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prueba la generación de texto ===\n",
      "\n",
      "Escribe un comienzo para generar texto: Ests profesor me\n",
      "¿Cuántos caracteres quieres generar? (recomendado: 100-200): 100\n",
      "¿Cuántas variaciones quieres ver? (1-3): 2\n",
      "\n",
      "Generando texto...\n",
      "\n",
      "Texto generado 1:\n",
      "Ests profesor me dijo que se sentía como si fuera un niño y que estaba a punto de morir. Pero luego de hacer algunas de esas cosas, fue a la casa de su familia. En el camino, estaba en el coche, y luego fue atropellado por un automóvil. Se dio cuenta de que era el hijo de la familia, pero no puedo decir lo que pasó con él, por lo menos me ha dicho que había estado en los años anteriores. Era muy divertido.\n",
      "\n",
      "En julio\n",
      "--------------------------------------------------\n",
      "\n",
      "Texto generado 2:\n",
      "Ests profesor me cuesta mucho, pero me gusta el juego de béisbol y el béisbol. Es como si me fuera un gran fan de la serie de televisión de TNT.\n",
      "\n",
      "Eskimowitz se graduó en la Universidad de Chicago, donde se especializó en economía, como la cual se ha especializado en finanzas. Se graduó con un título en gestión de empresas. Su primer trabajo de dirección fue como gerente de una empresa. En 2007, fue nombrado jefe de operaciones de un banco de inversión. Ha\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generar_texto_interactivo():\n",
    "    prompt = input(\"\\nEscribe un comienzo para generar texto: \")\n",
    "    longitud = int(input(\"¿Cuántos caracteres quieres generar? (recomendado: 100-200): \"))\n",
    "    variaciones = int(input(\"¿Cuántas variaciones quieres ver? (1-3): \"))\n",
    "\n",
    "    print(\"\\nGenerando texto...\")\n",
    "    generar_texto(prompt, max_length=longitud, num_return_sequences=variaciones)\n",
    "\n",
    "# Probar la función interactiva\n",
    "print(\"\\n=== Prueba la generación de texto ===\")\n",
    "generar_texto_interactivo()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPOrSnOU+F/MTPJNytdekim",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
