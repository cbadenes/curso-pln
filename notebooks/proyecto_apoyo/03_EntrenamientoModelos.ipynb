{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "P0lqa6x_qDaN",
   "metadata": {
    "id": "P0lqa6x_qDaN"
   },
   "source": [
    "#Dependencias Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wEzr7lCQTwTd",
   "metadata": {
    "id": "wEzr7lCQTwTd"
   },
   "outputs": [],
   "source": [
    "# Instalar las dependencias necesarias\n",
    "!pip install transformers\n",
    "!pip install -U datasets\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aAmnY1vd122g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAmnY1vd122g",
    "outputId": "3992fb33-52a7-4791-8950-02d08829ab37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KaThXfV33_QP",
   "metadata": {
    "id": "KaThXfV33_QP"
   },
   "source": [
    "#Generación de Texto (AutoModeloForCausalLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ccgt8Elpzoic",
   "metadata": {
    "id": "Ccgt8Elpzoic"
   },
   "source": [
    "##Antes de Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jpj0k9md4HPz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jpj0k9md4HPz",
    "outputId": "37f84742-c2af-4628-c34e-63b1a5aa87d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estopa es una banda española conocida por su estilo hard rock y su sonido punk. Formada en 1982 en Oviedo por Estel Mena y Juan Pablo Estel. Entre 1987 y 1993 estuvo integrada por Estel Mena, el bajista de la banda, que pasaría luego a formar la banda Maná.\n",
      "\n",
      "Historia \n",
      "\n",
      "Estoló fue\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Dataset con texto continuo sobre Estopa\n",
    "datos = [\n",
    "    {\"text\": \"Estopa es un dúo musical español formado por los hermanos David y José Manuel Muñoz. La banda se fundó en 1999 en Cornellà de Llobregat, Barcelona. Su estilo musical combina flamenco, rock y rumba catalana, creando un sonido único que los ha llevado a la fama.\"},\n",
    "    {\"text\": \"El primer álbum de Estopa, titulado 'Estopa', se lanzó en  1999 y fue un éxito inmediato, vendiendo más de un millón de copias. Canciones como 'La raja de tu falda' y 'Como Camarón' se convirtieron en clásicos.\"},\n",
    "    {\"text\": \"A lo largo de su carrera, Estopa ha lanzado más de 10 discos de estudio, manteniendo su característico estilo y evolucionando con nuevas influencias. Su álbum 'Destrangis' consolidó aún más su éxito con canciones como 'Vino tinto'.\"},\n",
    "    {\"text\": \"Estopa ha ganado numerosos premios, incluidos los Premios Ondas y los 40 Principales, que reconocen su contribución a la música española. Sus conciertos son conocidos por su energía y conexión con el público.\"},\n",
    "    {\"text\": \"La ciudad natal de los hermanos, Cornellà de Llobregat, influyó profundamente en su música. La mezcla cultural y las tradiciones flamencas del lugar se reflejan en sus letras y melodías.\"},\n",
    "    {\"text\": \"Además de su música, Estopa es conocido por sus letras llenas de humor y referencias cotidianas. Estas características los han hecho destacar y conectar con una audiencia amplia y diversa.\"},\n",
    "    {\"text\": \"Estopa continúa siendo una de las bandas más queridas en España, manteniendo su esencia mientras exploran nuevas direcciones en su música. Su legado perdurará como un símbolo de creatividad y autenticidad en la música española.\"}\n",
    "]\n",
    "\n",
    "# Crear un Dataset compatible con Hugging Face\n",
    "dataset = Dataset.from_list(datos)\n",
    "\n",
    "# Cargar el modelo y el tokenizador preentrenados en español\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "modelo = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizador = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Usar eos_token_id como pad_token_id\n",
    "tokenizador.pad_token = tokenizador.eos_token\n",
    "modelo.config.pad_token_id = tokenizador.eos_token_id\n",
    "\n",
    "# Tokenizar el dataset\n",
    "def procesar_datos(ejemplo):\n",
    "    tokenizado = tokenizador(\n",
    "    ejemplo[\"text\"],              # El texto de entrada que se va a tokenizar\n",
    "    max_length=128,               # Máxima longitud de la secuencia tokenizada\n",
    "    truncation=True,              # Recorta el texto si supera max_length\n",
    "    padding=\"max_length\",         # Rellena con tokens [PAD] hasta alcanzar max_length\n",
    "    return_tensors=\"pt\"           # Devuelve tensores de PyTorch (también puede ser \"tf\" o \"np\")\n",
    ")\n",
    "\n",
    "    return {key: tensor.squeeze() for key, tensor in tokenizado.items()}\n",
    "\n",
    "dataset_procesado = dataset.map(procesar_datos)\n",
    "\n",
    "# Usar DataCollatorForLanguageModeling (para gestionar el padding correctamente)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizador, mlm=False  # mlm=False porque es modelado causal, no enmascarado\n",
    ")\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "argumentos = TrainingArguments(\n",
    "    output_dir=\"./resultados\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"  # Desactiva W&B\n",
    ")\n",
    "\n",
    "# Ajustar las dimensiones del modelo al tokenizador (porque hemos añadido un nuevo token, el de padding)\n",
    "modelo.resize_token_embeddings(len(tokenizador))\n",
    "\n",
    "# Crear el Trainer\n",
    "trainer = Trainer(\n",
    "    model=modelo,\n",
    "    args=argumentos,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_procesado,\n",
    "    tokenizer=tokenizador,\n",
    "    eval_dataset=dataset_procesado\n",
    ")\n",
    "\n",
    "def generar_texto(prompt, modelo, tokenizador, max_length=100):\n",
    "    \"\"\"\n",
    "    Función para generar texto con el modelo actual.\n",
    "    \"\"\"\n",
    "    inputs = tokenizador(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    modelo.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = modelo.generate(**inputs, max_length=max_length, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "    return tokenizador.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Prueba antes del ajuste fino\n",
    "prompt_test = \"Estopa es una banda española conocida por\"\n",
    "print(\"\\n🔹 **Generación de texto ANTES del ajuste fino:**\")\n",
    "print(generar_texto(prompt_test, modelo, tokenizador))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "--bgFz-w5NII",
   "metadata": {
    "id": "--bgFz-w5NII"
   },
   "source": [
    "##Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kFIdsAsI5Mr5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "kFIdsAsI5Mr5",
    "outputId": "a49496af-7150-4082-e358-82d22b099df9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.699319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.288860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.191700</td>\n",
       "      <td>0.172364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=1.026526893178622, metrics={'train_runtime': 42.1278, 'train_samples_per_second': 0.498, 'train_steps_per_second': 0.285, 'total_flos': 16684615729152.0, 'train_loss': 1.026526893178622, 'epoch': 3.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-S03Tn0m5H_w",
   "metadata": {
    "id": "-S03Tn0m5H_w"
   },
   "source": [
    "##Después del Ajuste fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ltc-htSH5HMH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltc-htSH5HMH",
    "outputId": "c8760ead-6013-4137-b8e5-941e8f6521f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **Generación de texto DESPUÉS del ajuste fino:**\n",
      "Estopa es una banda española conocida por sus letras llenas de humor y referencias cotidianas. Comenzaron a sonar en los años 90, siendo un de los grupos más queridos y queridos en España. Su legado perdurará como un símbolo de creatividad y autenticidad en la música española. Su álbum 'Qué bueno' se llevó a una raja de éxitos in\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n **Generación de texto DESPUÉS del ajuste fino:**\")\n",
    "print(generar_texto(prompt_test, modelo, tokenizador))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131hap-eWVSn",
   "metadata": {
    "id": "131hap-eWVSn"
   },
   "source": [
    "#2. Clasificación de Texto (AutoModelForSequenceClassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lsqvzYVWg4UQ",
   "metadata": {
    "id": "lsqvzYVWg4UQ"
   },
   "source": [
    "##Antes de ajuste fino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kiQUHPDcg3Hf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiQUHPDcg3Hf",
    "outputId": "907b76d9-e806-4dbc-f189-129af5eb691c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.33      0.50      0.40         2\n",
      "    Positive       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.25         4\n",
      "   macro avg       0.17      0.25      0.20         4\n",
      "weighted avg       0.17      0.25      0.20         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "\n",
    "# Tokenizador y modelo preentrenado\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Positivo y Negativo\n",
    "\n",
    "#EVALUACIÓN\n",
    "texts = [\"Amazing movie!\", \"Terrible plot.\", \"Loved the characters!\", \"Not my taste.\"]\n",
    "true_labels = [1, 0, 1, 0]  # Etiquetas reales\n",
    "\n",
    "# Predicciones\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "predicted_classes = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(true_labels, predicted_classes)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(true_labels, predicted_classes, target_names=[\"Negative\", \"Positive\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3qs340G_u95D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qs340G_u95D",
    "outputId": "77b43d75-63d1-4a5e-8134-5bc66000c776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASES PREDICHAS: [0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"CLASES PREDICHAS:\", predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O9C8pyXAh-TN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9C8pyXAh-TN",
    "outputId": "c2088129-a878-4ce6-bb0a-0203dc4bad74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: This movie is amazing, I loved it!\n",
      "Predicción: Negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Texto de prueba\n",
    "text = \"This movie is amazing, I loved it!\"\n",
    "\n",
    "# Tokenización y predicción\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Mover las entradas al mismo dispositivo que el modelo (GPU si está disponible)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # predicciones sin normalizar, es decir, los valores antes de aplicar una función como softmax.\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Mostrar resultado\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}  # Cambia según las etiquetas de tu modelo\n",
    "print(f\"Texto: {text}\")\n",
    "print(f\"Predicción: {label_map[predicted_class]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Rpo7D2TiAI4",
   "metadata": {
    "id": "0Rpo7D2TiAI4"
   },
   "source": [
    "##AJUSTE FINO CON DATASET DE IMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "msAxe2GcTwTe",
   "metadata": {
    "id": "msAxe2GcTwTe"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el dataset IMDb (análisis de sentimientos)\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "# Tokenizador y modelo preentrenado\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Positivo y Negativo\n",
    "\n",
    "# Preprocesar datos\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Usar DataCollator para hacer padding dinámico\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Configurar el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000)),\n",
    "    eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dTANfgKfwbtB",
   "metadata": {
    "id": "dTANfgKfwbtB"
   },
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z8sY6LyPXIwz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8sY6LyPXIwz",
    "outputId": "566e9b0a-00c6-46fb-8e7c-3b77fd903a0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: This movie is amazing, I loved it!\n",
      "Predicción: Positive\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Texto de prueba\n",
    "text = \"This movie is amazing, I loved it!\"\n",
    "\n",
    "# Tokenización y predicción\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Mover las entradas al mismo dispositivo que el modelo (GPU si está disponible)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # predicciones sin normalizar, es decir, los valores antes de aplicar una función como softmax.\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Mostrar resultado\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}  # Cambia según las etiquetas de tu modelo\n",
    "print(f\"Texto: {text}\")\n",
    "print(f\"Predicción: {label_map[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8tnEDi-eetQ4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tnEDi-eetQ4",
    "outputId": "c9f47763-c68f-458a-8963-d2d0c4ae30a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00         2\n",
      "    Positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#EVALUACIÓN\n",
    "texts = [\"Amazing movie!\", \"Terrible plot.\", \"Loved the characters!\", \"Not my taste.\"]\n",
    "true_labels = [1, 0, 1, 0]  # Etiquetas reales\n",
    "\n",
    "# Predicciones\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "predicted_classes = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(true_labels, predicted_classes)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(true_labels, predicted_classes, target_names=[\"Negative\", \"Positive\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2xjuhQLgV-qm",
   "metadata": {
    "id": "2xjuhQLgV-qm"
   },
   "source": [
    "#2. Reconocimiento de Entidades Nombradas (AutoModelForTokenClassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6YF96ZpjzbY7",
   "metadata": {
    "id": "6YF96ZpjzbY7"
   },
   "source": [
    "##Antes de Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UM0CR5M4TwTf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "a2a411cecd2945648a242e31ac01c3f4",
      "0360a3748c384c03a8c0ed6f4f4c4dc0",
      "35fab11e06734a66b5ee1fceda2a97ab",
      "9cb0ad0703d94e2b90eb7380267d629a",
      "ac6bdcf263f14eaaa76bc0ad97170018",
      "28e324dea91e4d40b148619669b70361",
      "8674e8e77faf473c864b141efd2549e2",
      "6a51823a124c4ff19d73640a18c40f19",
      "21621fd2a943401fbd4be8df5d4c5ac5",
      "73006f85805647bd87f6e5b61c906305",
      "891bb4789ae24bf0b7da3b360ffccf2d"
     ]
    },
    "id": "UM0CR5M4TwTf",
    "outputId": "e2b0c78f-533d-4d45-dbe0-3716aa43382a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a411cecd2945648a242e31ac01c3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'My', 'name', 'is', 'Wolfgang', 'and', 'I', 'live', 'in', 'Berlin', '[SEP]']\n",
      "Etiquetas predichas: ['I-ORG', 'I-ORG', 'I-MISC', 'I-ORG', 'I-MISC', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-MISC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-820689317>:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el dataset CoNLL-2003\n",
    "#-- El dataset CoNLL-2003 es un conjunto de datos muy utilizado para tareas de Reconocimiento de Entidades Nombradas (Named Entity Recognition, NER) en Procesamiento del Lenguaje Natural.\n",
    "#-- Fue introducido en la conferencia CoNLL-2003 shared task, organizada por la conferencia de la Association for Computational Linguistics (ACL).\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=\"True\")\n",
    "\n",
    "# Tokenizador y modelo preentrenado\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(dataset[\"train\"].features[\"ner_tags\"].feature.names))\n",
    "\n",
    "# Preprocesar datos\n",
    "def preprocess_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = [-100 if word_id is None else label[word_id] for word_id in word_ids]\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Usar DataCollator para alineación de etiquetas\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Configurar el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000)),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(500)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Texto de prueba\n",
    "text = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "# Tokenización y predicción\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "# Obtener las etiquetas predichas\n",
    "predicted_labels = [dataset[\"train\"].features[\"ner_tags\"].feature.names[p] for p in predictions[0].tolist()]\n",
    "\n",
    "# Imprimir los tokens y sus etiquetas predichas\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "print(\"Etiquetas predichas:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4S5xS9AcD6zT",
   "metadata": {
    "id": "4S5xS9AcD6zT"
   },
   "source": [
    "##Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81rtQrOED2ST",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "81rtQrOED2ST",
    "outputId": "b5052c81-7fb6-435e-ad26-df1276f98644"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.149108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.124064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.100066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.16891073608398438, metrics={'train_runtime': 29.0765, 'train_samples_per_second': 206.352, 'train_steps_per_second': 12.897, 'total_flos': 152435476445472.0, 'train_loss': 0.16891073608398438, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g40vv2BRD8Ef",
   "metadata": {
    "id": "g40vv2BRD8Ef"
   },
   "source": [
    "##Después de Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "RYv32iG7WYUE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYv32iG7WYUE",
    "outputId": "3c8b9f1c-98b3-43a8-90af-10863293582a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'My', 'name', 'is', 'Wolfgang', 'and', 'I', 'live', 'in', 'Berlin', '[SEP]']\n",
      "Etiquetas predichas: ['O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Texto de prueba\n",
    "text = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "# Tokenización y predicción\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "# Obtener las etiquetas predichas\n",
    "predicted_labels = [dataset[\"train\"].features[\"ner_tags\"].feature.names[p] for p in predictions[0].tolist()]\n",
    "\n",
    "# Imprimir los tokens y sus etiquetas predichas\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "print(\"Etiquetas predichas:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9N1xCWdIWYpn",
   "metadata": {
    "id": "9N1xCWdIWYpn"
   },
   "source": [
    "#3. Preguntas y Respuestas (AutoModelForQuestionAnswering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MO_N7fboLKJ1",
   "metadata": {
    "id": "MO_N7fboLKJ1"
   },
   "source": [
    "##Antes de Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "K5nSUvn1TwTg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5nSUvn1TwTg",
    "outputId": "4611a0e6-4cbf-4503-d5ed-4b96dd350a2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: Where is Badajoz placed?\n",
      "Contexto: Badajoz is a city located in the autonomous community of Extremadura, in southwestern Spain, near the Portuguese border.\n",
      "It is the largest city in the region by population and surface area. Founded during the Moorish period, Badajoz preserves\n",
      "important historical remains, such as its massive Alcazaba, one of the largest Arab citadels in Europe. The city's architecture\n",
      "also reflects Gothic and Renaissance influences, visible in its cathedral and old quarters. Badajoz hosts several cultural events,\n",
      "including the Carnival of Badajoz, considered one of the most important in Spain. It is also an academic and economic center,\n",
      "home to one of the campuses of the University of Extremadura. The city has a hot-summer Mediterranean climate, with mild winters\n",
      "and very hot, dry summers. The local economy relies on services, trade, agriculture, and cross-border cooperation with Portugal,\n",
      "particularly with the nearby city of Elvas. Extremadura as a region is known for its natural parks, traditional cuisine, and\n",
      "products such as Iberian ham and paprika from La Vera.\n",
      "Respuesta: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-57-2297793452>:81: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el dataset SQuAD v2\n",
    "#--El dataset SQuAD v2 (Stanford Question Answering Dataset, versión 2.0) es un conjunto de datos muy influyente en el campo del Question Answering (QA), específicamente para tareas de pregunta-respuesta extractiva sobre texto.\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Tokenizador y modelo preentrenado\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Preprocesar datos\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=True\n",
    "    )\n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = inputs[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answer = answers[sample_index]\n",
    "        if len(answer[\"answer_start\"]) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answer[\"text\"][0])\n",
    "            token_start_index = 0\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "            if offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_positions.append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_positions.append(token_end_index + 1)\n",
    "            else:\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Usar DefaultDataCollator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest', max_length=512) #max_lenght para evitar error de memoria\n",
    "# Configuración del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Configurar el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(6000)),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(750)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Ejemplo de pregunta y contexto\n",
    "context = \"\"\"Badajoz is a city located in the autonomous community of Extremadura, in southwestern Spain, near the Portuguese border.\n",
    "It is the largest city in the region by population and surface area. Founded during the Moorish period, Badajoz preserves\n",
    "important historical remains, such as its massive Alcazaba, one of the largest Arab citadels in Europe. The city's architecture\n",
    "also reflects Gothic and Renaissance influences, visible in its cathedral and old quarters. Badajoz hosts several cultural events,\n",
    "including the Carnival of Badajoz, considered one of the most important in Spain. It is also an academic and economic center,\n",
    "home to one of the campuses of the University of Extremadura. The city has a hot-summer Mediterranean climate, with mild winters\n",
    "and very hot, dry summers. The local economy relies on services, trade, agriculture, and cross-border cooperation with Portugal,\n",
    "particularly with the nearby city of Elvas. Extremadura as a region is known for its natural parks, traditional cuisine, and\n",
    "products such as Iberian ham and paprika from La Vera.\"\"\"\n",
    "question = \"Where is Badajoz placed?\"\n",
    "# Tokenizar la pregunta y el contexto\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Realizar la inferencia\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtener las respuestas\n",
    "answer_start_index = torch.argmax(outputs.start_logits)\n",
    "answer_end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "\n",
    "predict_answer_tokens = inputs[\"input_ids\"][0, answer_start_index : answer_end_index + 1]\n",
    "answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(f\"Pregunta: {question}\")\n",
    "print(f\"Contexto: {context}\")\n",
    "print(f\"Respuesta: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tWQIYTcgEyD0",
   "metadata": {
    "id": "tWQIYTcgEyD0"
   },
   "source": [
    "##Entrenar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6KrNRIHNEw8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "6KrNRIHNEw8a",
    "outputId": "902e7788-67fd-4158-b668-8f87c7cf061c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 04:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.890600</td>\n",
       "      <td>1.882181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.629400</td>\n",
       "      <td>1.981344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.070600</td>\n",
       "      <td>1.998787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.612900</td>\n",
       "      <td>2.429250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>2.744407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=1.2711611165364582, metrics={'train_runtime': 277.9042, 'train_samples_per_second': 107.951, 'train_steps_per_second': 13.494, 'total_flos': 2939694750720000.0, 'train_loss': 1.2711611165364582, 'epoch': 5.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H-CjfV4oEzeq",
   "metadata": {
    "id": "H-CjfV4oEzeq"
   },
   "source": [
    "##Después de Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "QaLjPYzbXRsu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QaLjPYzbXRsu",
    "outputId": "8e36a39a-89d4-4b03-edcf-31ba18169cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: where is Badajoz located?\n",
      "Contexto: Badajoz is a city located in the autonomous community of Extremadura, in southwestern Spain, near the Portuguese border.\n",
      "It is the largest city in the region by population and surface area. Founded during the Moorish period, Badajoz preserves\n",
      "important historical remains, such as its massive Alcazaba, one of the largest Arab citadels in Europe. The city's architecture\n",
      "also reflects Gothic and Renaissance influences, visible in its cathedral and old quarters. Badajoz hosts several cultural events,\n",
      "including the Carnival of Badajoz, considered one of the most important in Spain. It is also an academic and economic center,\n",
      "home to one of the campuses of the University of Extremadura. \n",
      "The city has a hot-summer Mediterranean climate, with mild winters\n",
      "and very hot, dry summers. The local economy relies on services, trade, agriculture, and cross-border cooperation with Portugal,\n",
      "particularly with the nearby city of Elvas. \n",
      "Extremadura as a region is known for its natural parks, traditional cuisine, and\n",
      "products such as Iberian ham and paprika from La Vera.\n",
      "Respuesta: [CLS] where is badajoz located? [SEP] badajoz is a city located in the autonomous community of extremadura, in southwestern spain\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de pregunta y contexto\n",
    "context = \"\"\"Badajoz is a city located in the autonomous community of Extremadura, in southwestern Spain, near the Portuguese border.\n",
    "It is the largest city in the region by population and surface area. Founded during the Moorish period, Badajoz preserves\n",
    "important historical remains, such as its massive Alcazaba, one of the largest Arab citadels in Europe. The city's architecture\n",
    "also reflects Gothic and Renaissance influences, visible in its cathedral and old quarters. Badajoz hosts several cultural events,\n",
    "including the Carnival of Badajoz, considered one of the most important in Spain. It is also an academic and economic center,\n",
    "home to one of the campuses of the University of Extremadura.\n",
    "The city has a hot-summer Mediterranean climate, with mild winters\n",
    "and very hot, dry summers. The local economy relies on services, trade, agriculture, and cross-border cooperation with Portugal,\n",
    "particularly with the nearby city of Elvas.\n",
    "Extremadura as a region is known for its natural parks, traditional cuisine, and\n",
    "products such as Iberian ham and paprika from La Vera.\"\"\"\n",
    "question = \"where is Badajoz located?\"\n",
    "# Tokenizar la pregunta y el contexto\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Realizar la inferencia\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtener las respuestas\n",
    "answer_start_index = torch.argmax(outputs.start_logits)\n",
    "answer_end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "predict_answer_tokens = inputs[\"input_ids\"][0, answer_start_index : answer_end_index + 1]\n",
    "answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(f\"Pregunta: {question}\")\n",
    "print(f\"Contexto: {context}\")\n",
    "print(f\"Respuesta: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ZtfbyLRA88Mo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtfbyLRA88Mo",
    "outputId": "572caf79-9a3e-4f1b-82c9-851177fa90cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: which is the most important cultural event in Badajoz? \n",
      "Respuesta: carnival\n"
     ]
    }
   ],
   "source": [
    "question = \"which is the most important cultural event in Badajoz? \"\n",
    "# Tokenizar la pregunta y el contexto\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Realizar la inferencia\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtener las respuestas\n",
    "answer_start_index = torch.argmax(outputs.start_logits)\n",
    "answer_end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "predict_answer_tokens = inputs[\"input_ids\"][0, answer_start_index : answer_end_index + 1]\n",
    "answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(f\"Pregunta: {question}\")\n",
    "print(f\"Respuesta: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn05gQ2GZJsj",
   "metadata": {
    "id": "nn05gQ2GZJsj"
   },
   "source": [
    "#AutoModelForMaskedLM.from_pretrained (Masked Language Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5K-TXCF77wjf",
   "metadata": {
    "id": "5K-TXCF77wjf"
   },
   "outputs": [],
   "source": [
    "pip install wikipedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C67aWuCaKDFM",
   "metadata": {
    "id": "C67aWuCaKDFM"
   },
   "source": [
    "##Antes de ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "Uw-LKhTLlMh7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uw-LKhTLlMh7",
    "outputId": "90eadba0-4137-4334-bef7-cfffa6cc00c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del ajuste fino:\n",
      "Texto: The capital city of Extremadura is [MASK].\n",
      "Predicción: madrid\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-3129552404>:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import wikipedia\n",
    "\n",
    "wikipedia.set_lang(\"en\")\n",
    "# Dataset para Masked LM\n",
    "extremadura_text = wikipedia.page(\"Extremadura\").content\n",
    "ds = Dataset.from_dict({\"text\": [extremadura_text]*1000})\n",
    "\n",
    "\n",
    "# Tokenizador y modelo\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Tokenización\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "\n",
    "# DataCollator para Masked LM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.5)\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Comprobar rendimiento antes del ajuste fino\n",
    "text = \"The capital city of Extremadura is [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predicted_token_id = outputs.logits[0, inputs.input_ids[0].tolist().index(tokenizer.mask_token_id)].argmax().item()\n",
    "print(\"Antes del ajuste fino:\")\n",
    "print(f\"Texto: {text}\")\n",
    "print(f\"Predicción: {tokenizer.decode(predicted_token_id)}\\n\")\n",
    "\n",
    "# Entrenamiento\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0q1I7VnEzPhq",
   "metadata": {
    "id": "0q1I7VnEzPhq"
   },
   "source": [
    "##Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "uyeT216FyFb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "uyeT216FyFb2",
    "outputId": "e6d91526-55e6-46b4-aa41-879f12335c9b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 00:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.013320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.005356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=315, training_loss=0.10081304519895523, metrics={'train_runtime': 58.7365, 'train_samples_per_second': 85.126, 'train_steps_per_second': 5.363, 'total_flos': 329006016000000.0, 'train_loss': 0.10081304519895523, 'epoch': 5.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uCgv1IGYKFbA",
   "metadata": {
    "id": "uCgv1IGYKFbA"
   },
   "source": [
    "##Después del Ajuste fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2WTdS4g4SnOX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2WTdS4g4SnOX",
    "outputId": "c17de504-ae86-426f-9877-09412eb5cdc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después del ajuste fino:\n",
      "Texto: The capital city of Extremadura is [MASK].\n",
      "Predicción: merida\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Comprobar rendimiento después del ajuste fino\n",
    "outputs = model(**inputs)\n",
    "predicted_token_id = outputs.logits[0, inputs['input_ids'][0].tolist().index(tokenizer.mask_token_id)].argmax().item()\n",
    "print(\"Después del ajuste fino:\")\n",
    "print(f\"Texto: {text}\")\n",
    "print(f\"Predicción: {tokenizer.decode(predicted_token_id)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k8ZdxG-wZQXN",
   "metadata": {
    "id": "k8ZdxG-wZQXN"
   },
   "source": [
    "#3. AutoModelForMultipleChoice (Selección Múltiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10HqzvvYKIbT",
   "metadata": {
    "id": "10HqzvvYKIbT"
   },
   "source": [
    "##Antes del ajuste fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "qcKZRxIeIrGv",
   "metadata": {
    "id": "qcKZRxIeIrGv"
   },
   "outputs": [],
   "source": [
    "#borro todas la variables\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3Yvwl53WJ9Of",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Yvwl53WJ9Of",
    "outputId": "12abd488-c7a7-47a3-d6ff-bd1b4c61e01c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-107-1996011700>:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del ajuste fino\n",
      "Logits: tensor([[-0.1461, -0.1701,  0.0089, -0.1836]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Opción elegida: She called her teacher to ask about homework.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# 1. Cargar el dataset PIQA\n",
    "dataset = load_dataset(\"piqa\")\n",
    "\n",
    "# 2. Usar un modelo base\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
    "\n",
    "# 3. Preprocesado: convertir en formato multiple-choice\n",
    "def preprocess_function(examples):\n",
    "    # Cada ejemplo tiene un 'goal' y dos opciones: 'sol1' y 'sol2'\n",
    "    first_sentences = []\n",
    "    second_sentences = []\n",
    "    for goal, sol1, sol2 in zip(examples[\"goal\"], examples[\"sol1\"], examples[\"sol2\"]):\n",
    "        first_sentences.extend([goal, goal])\n",
    "        second_sentences.extend([sol1, sol2])\n",
    "\n",
    "    # Tokenize the questions and choices\n",
    "    tokenized = tokenizer(first_sentences, second_sentences, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # Agrupar en pares (batch_size, num_choices, seq_len)\n",
    "    result = {\n",
    "        k: [tokenized[k][i:i+2] for i in range(0, len(tokenized[k]), 2)]\n",
    "        for k in tokenized\n",
    "    }\n",
    "    result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "# Aplicamos el preprocesado a train y validation\n",
    "encoded_train = dataset[\"train\"].select(range(2000)).map(preprocess_function, batched=True)\n",
    "encoded_valid = dataset[\"validation\"].select(range(500)).map(preprocess_function, batched=True)\n",
    "\n",
    "# DatasetWrapper para que funcione con Trainer\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: bool = True\n",
    "    max_length: int = None\n",
    "\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "\n",
    "        flattened_features = []\n",
    "        for feature in features:\n",
    "            for i in range(num_choices):\n",
    "                flattened_features.append(\n",
    "                    {k: feature[k][i] for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] if k in feature}\n",
    "                )\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        batch = {\n",
    "            k: v.view(batch_size, num_choices, -1) for k, v in batch.items()\n",
    "        }\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# 4. Argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./piqa_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Comprobar logits antes del entrenamiento\n",
    "prompt = [\"Laura went to the kitchen and opened the fridge. She saw that the milk was expired.\"]\n",
    "choices = [\n",
    "    \"She threw it away and took a juice instead.\",         # coherente\n",
    "    \"She painted the fridge green.\",                       # absurda\n",
    "    \"She called her teacher to ask about homework.\",       # irrelevante\n",
    "    \"She put the expired milk in her backpack.\"            # extraño e incorrecto\n",
    "]\n",
    "inputs = tokenizer(prompt * len(choices), choices, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "# Re-dimensionamos para que sea (batch_size=1, num_choices=4, seq_length)\n",
    "for k in inputs:\n",
    "    inputs[k] = inputs[k].unsqueeze(0)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
    "best_choice = choices[predicted_idx]\n",
    "print(\"Antes del ajuste fino\")\n",
    "print(f\"Logits: {outputs.logits}\\n\")\n",
    "print(\"Opción elegida:\", best_choice)\n",
    "\n",
    "# Entrenamos con Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XsWVz1XmzBC_",
   "metadata": {
    "id": "XsWVz1XmzBC_"
   },
   "source": [
    "##Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3ro2PJRvy-m_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "3ro2PJRvy-m_",
    "outputId": "a20406b9-8277-426e-aa88-efdc871ce564"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 02:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.683393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.634900</td>\n",
       "      <td>0.704613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.468100</td>\n",
       "      <td>0.736609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.771213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.5414263687133789, metrics={'train_runtime': 153.0087, 'train_samples_per_second': 65.356, 'train_steps_per_second': 13.071, 'total_flos': 1315543464960000.0, 'train_loss': 0.5414263687133789, 'epoch': 5.0})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O451FIqBKN4-",
   "metadata": {
    "id": "O451FIqBKN4-"
   },
   "source": [
    "##Después del ajuste fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "BpLdTpqiAKLB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BpLdTpqiAKLB",
    "outputId": "a0b4455c-efa1-4787-9ba5-eed3a68034fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después del ajuste fino:\n",
      "Logits: tensor([[ 0.8619,  0.1487, -2.7114,  0.3739]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Opción elegida: She threw it away and took a juice instead.\n"
     ]
    }
   ],
   "source": [
    "# Comprobar rendimiento después del ajuste fino\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
    "best_choice = choices[predicted_idx]\n",
    "print(\"Después del ajuste fino:\")\n",
    "print(f\"Logits: {outputs.logits}\\n\")\n",
    "print(\"Opción elegida:\", best_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9spSksROZYR4",
   "metadata": {
    "id": "9spSksROZYR4"
   },
   "source": [
    "#4. AutoModelForSeq2SeqLM.from_pretrained (Traducción/Secuencia a Secuencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HGgrBd6QKSdv",
   "metadata": {
    "id": "HGgrBd6QKSdv"
   },
   "source": [
    "##Antes del ajuste fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "lZipDlMmllkg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476,
     "referenced_widgets": [
      "b465ce49c3664831b75bf5da55f07924",
      "17ebe2cc3d964c6ba3fbc7a599d4298f",
      "5d941b9a82854cbaa4a92541cbe02423",
      "977195537bfa4fdb9a3b29bafc78c754",
      "b0dc87c97c3d46c18dc59f58e5369f47",
      "d29b3ae9a3834a6097efabbf045c92bf",
      "20e4404437034b6ab81413f8e40982f8",
      "c59de29c0b0b49ee83541c9616b401ba",
      "4b6415b7d321471bbdf0feb658e11629",
      "c228c6a5cb3e4fb4a8dc1bcab3a1beb8",
      "b93cd88dd42a41e19973dcd184f19be1",
      "882305a2acdc41ebbdb02cd7ba97e82a",
      "60e1e992884b411c9be2a20e16b60999",
      "a5d00cdefcea40cea760ae18e267d93b",
      "2d4ef9cc146a4f4ab06db159f78544dd",
      "57805feb85c34f559aaf17b1c0228e8b",
      "5e30ede5b64343d09cd8a77e1d5b9719",
      "5d9d42f2e5af465e873278cb8ef4d432",
      "eee67b80f4e54fbb9d11888e6114e4b1",
      "2c1c4f1ecadc469c8bc7e70e62f01f93",
      "031eb224e9094aacaadeef7889e2cd42",
      "189710cfb3d54af4a5b9a84487b11956",
      "596914882da040ccade4334b12d23303",
      "8570643f55a148f4ae39009b37a9b203",
      "29a519abcb8a4203b694a162cd6d40a5",
      "a71f81f3b6c24ccfb922bbb3ab3de032",
      "6f009143b83147ad848c18b3c96a5d61",
      "036c456aa39243c2816a5fe4def6f584",
      "7a746c51cab744a08b0476797e955764",
      "639c023ca8274a55a176463b697aae25",
      "5ae94f90881c4dc59d82aca4e7d2a9f4",
      "c4393a9c8c9c44d4b7f8fe3aa38e0f71",
      "77338560b6794bf28abf3a59385270b1",
      "cb53c61a0b8943ad8271f72b025dfe15",
      "851eb5061a714b74bc26d971d7a9bc0e",
      "acb621536953429fafd5981344e1b88f",
      "ee85f5ec3f444cc29294670d551fb7d4",
      "29347b55e2d846eead938ec17b8007e8",
      "ffac99be67964628932cc2dbdc7b014f",
      "5b70438893d64960adc849fd546f2f09",
      "f0180735f7134e2f82e165fb71c0803a",
      "6d4d2b2af4fb42f0b3069151ad1b6d05",
      "33119475f2b248cbb027081a39e9748d",
      "f846946eeccd49829967f9de49f28f3d",
      "e79670a5848b4669a7de6252f4d11ee2",
      "19111d9c8898436d9ba9fa5d7b3ea5bb",
      "6633733c62c84cd2b6055a3cf930f7af",
      "09abb8cfcb1c438c900ae495cc8e8883",
      "e05ad0191d7d47168d6b630109c64097",
      "7b99f8bdc40c4294bbbd1079311fa78f",
      "14c79e7d2de0435288fe411a8617c502",
      "af32b5b1913c4321b5da99ae96526860",
      "fa7178ee2560448b82003754726272e1",
      "af5dc34b17b7420a906e364b536b1270",
      "290921cd68674ee581ee0007d95cfaac",
      "9c6cd1307f1d462d959999f4a2681f3a",
      "72582ff075604ecfbaa974b8f24fdf05",
      "9867a80d4890467692b9d0f12f9e8e7f",
      "7bc68e1d469e449ca9b1388945a04bd4",
      "071fa86dc1df40e2bfb0ca03114f9654",
      "37a5f3cb94ab4d52ba1c8941a098ffb9",
      "1b228cd94aed45e099ad5e701e12b7e7",
      "add082aa08a649d19756e6bca732bd1c",
      "b708d28b55e44f83b02e5b36bc601798",
      "be91e955c1024841ac8156f3f1310f7e",
      "3958f736693d4329afc403dfd0c907d7",
      "e3bd48e755d642828b9c55f92c688a44",
      "060a26074f7244169cbdc8c9915dbb2f",
      "623e9201a3144f0683baef2e0f5818c2",
      "aaca669ac042482796438dd0c436fb4e",
      "b029a17ab7b64c29a37978af546a5aec",
      "90bc5b3af896428f92145d073558e95a",
      "9f8d4d9a9650455e9b0185473177d38d",
      "51e27e58e66b41cf86934248d5538aaf",
      "ee671209a31f4489842b8b74875dcc78",
      "cd2c9511393b456f843c3a5aebe8893a",
      "cade2fbdf18c4e7d9ab39f3fa55c9edc",
      "030b68b68cb14e2bbafbebf52e67fd14",
      "bb9685455e8740a5abdcbf01313b2aee",
      "9e190f7b332a45ef9f96fe401bab35fc",
      "cbd52b0d33b94f13b487b00bc1b90f7d",
      "3d2f10106812441ea36f1cffafa1d043",
      "050f39739952421e95288781e929d061",
      "6e38673f3c5d45c8b084ad27d720f0ac",
      "dbd8167b296c4e9fa17c93a913f99d5e",
      "6ef4bdaf6548449085e0087407406894",
      "9d9c2f1a44c249ddaf3b1103366863cc",
      "2c554786d3184d5d85277a207bc2e9f2",
      "9c871495d6664509b49ef2525536ef1b",
      "866a8fd88c5e4d05a0ce24be7b62bc0f",
      "21e13b6c80f74ba7a344f843e6ee09df",
      "d0e4e3dfcbdc4891a733af3344d42df2",
      "346db81167d644a8a5f0fac641deea55",
      "7892b4a22dd2407f9cb3c19c141a47d2",
      "0b932a8a9ad942ebaf9507f4536ef4d6",
      "5a7816396f88425d9e56b85e049b71e4",
      "53eb6407971e4665b1a3771639bda791",
      "9a896c86d6dd48a7b69c09bbcc2dfde0",
      "d068299a5c124a65ab32a5065f0e4adc",
      "a9db2689f31740b1861c7946eca38b77",
      "1220e4a0f0534f67b2b41e2692f77e06",
      "3fc956422af74731b0fdbf8f3383fe11",
      "782d78534efd49198315d44ccf93db14",
      "684b56c12b2c4ba8a6e7e5878cbaaf8d",
      "bafec54e554e4f52926edf2a8304a337",
      "543d580a4ff248eebf82e48f275b8034",
      "dfb464a0046046f6bf7ce36d78113664",
      "0cafa7f4e77940e696fe2007e0c4d90a",
      "0da8ee5029344753a456e2e7e0299617",
      "92881e0c5f2d49cc8ad160e55f729da4",
      "b1783128c6f64a33b80dd1f9feacb70d",
      "c6b7de38ab88421b995ff807f06f7ab2",
      "0f93e1ba22744364bd30c36d72cb04b4",
      "963fb08ed79d4b5596282d54b188adec",
      "dcbff23628fb4ff68c3f8d7e9c6ea01a",
      "b940e46c89da486ab3b4608ff736750a",
      "1f6648b35f7d4c4591b2ca0575f2314e",
      "106e99f796ce4a98bd4a2cbae2a514cf",
      "76755af30bf24c22bd346343721b39eb",
      "d7b02dfd225c4346b9d1b6763d5da320",
      "c8822dd9d7c9441ca8432b69143fcfb3"
     ]
    },
    "id": "lZipDlMmllkg",
    "outputId": "ad146388-c55b-4c2c-db48-0be7c6990cc7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b465ce49c3664831b75bf5da55f07924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882305a2acdc41ebbdb02cd7ba97e82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/16.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596914882da040ccade4334b12d23303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/93470 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb53c61a0b8943ad8271f72b025dfe15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79670a5848b4669a7de6252f4d11ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6cd1307f1d462d959999f4a2681f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bd48e755d642828b9c55f92c688a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030b68b68cb14e2bbafbebf52e67fd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c871495d6664509b49ef2525536ef1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9db2689f31740b1861c7946eca38b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1783128c6f64a33b80dd1f9feacb70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del ajuste fino:\n",
      "Entrada: The book is on the table.\n",
      "Traducción: Das Buch ist auf dem Tisch.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-61-1363106736>:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar dataset de traducción\n",
    "dataset = load_dataset(\"opus_books\", \"en-es\")\n",
    "#Creamos split de validación (10% del train)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "dataset['validation'] = dataset['test']\n",
    "# Tokenizador y modelo\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Preprocesar datos para traducción\n",
    "def preprocess_function(examples):\n",
    "    # \"translation\" es un diccionario con {\"en\": ..., \"es\": ...}\n",
    "    # Recuperamos la lista de textos en inglés y español\n",
    "    en_texts = [item[\"en\"] for item in examples[\"translation\"]]\n",
    "    es_texts = [item[\"es\"] for item in examples[\"translation\"]]\n",
    "\n",
    "    # Construimos el prompt: \"translate English to Spanish: <texto_en>\"\n",
    "    inputs = [f\"translate English to Spanish: {text}\" for text in en_texts]\n",
    "    # El target será directamente el texto en español\n",
    "    targets = [text for text in es_texts]\n",
    "\n",
    "    # Tokenizamos entradas y salidas\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    labels = tokenizer(targets, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # Añadimos las labels al diccionario de tokens\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Usar DataCollator para Seq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Comprobar rendimiento antes del ajuste fino\n",
    "def test_translation(model, tokenizer, input_text):\n",
    "    inputs = tokenizer(f\"translate English to Spanish: {input_text}\", return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(**inputs, max_length=50)\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "print(\"Antes del ajuste fino:\")\n",
    "input_text = \"The book is on the table.\"\n",
    "print(f\"Entrada: {input_text}\")\n",
    "print(f\"Traducción: {test_translation(model, tokenizer, input_text)}\\n\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].shuffle(seed=42),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EKuYy_irzI3x",
   "metadata": {
    "id": "EKuYy_irzI3x"
   },
   "source": [
    "##Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "MyoH9BE1zMcY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "MyoH9BE1zMcY",
    "outputId": "6b9168ea-09c8-460a-be66-e7914b3c82d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15774' max='15774' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15774/15774 24:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.105300</td>\n",
       "      <td>0.994683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.052200</td>\n",
       "      <td>0.923904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.009000</td>\n",
       "      <td>0.905965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15774, training_loss=1.1174980832004753, metrics={'train_runtime': 1446.9772, 'train_samples_per_second': 174.411, 'train_steps_per_second': 10.901, 'total_flos': 8539018773921792.0, 'train_loss': 1.1174980832004753, 'epoch': 3.0})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9AqCp5c2KVvN",
   "metadata": {
    "id": "9AqCp5c2KVvN"
   },
   "source": [
    "##Después del Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "yIchufSxp8fj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIchufSxp8fj",
    "outputId": "ec51f072-78d8-4691-a49b-668781b6c0ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después del ajuste fino:\n",
      "Entrada: The book is on the table.\n",
      "Traducción: El libro está en la mesa.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprobar rendimiento después del ajuste fino\n",
    "\n",
    "print(\"Después del ajuste fino:\")\n",
    "print(f\"Entrada: {input_text}\")\n",
    "print(f\"Traducción: {test_translation(model, tokenizer, input_text)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
