{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "P0lqa6x_qDaN",
   "metadata": {
    "id": "P0lqa6x_qDaN"
   },
   "source": [
    "#Dependencias Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wEzr7lCQTwTd",
   "metadata": {
    "id": "wEzr7lCQTwTd"
   },
   "outputs": [],
   "source": [
    "# Instalar las dependencias necesarias\n",
    "!pip install transformers\n",
    "!pip install -U datasets\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aAmnY1vd122g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAmnY1vd122g",
    "outputId": "3992fb33-52a7-4791-8950-02d08829ab37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KaThXfV33_QP",
   "metadata": {
    "id": "KaThXfV33_QP"
   },
   "source": [
    "#Generaci칩n de Texto (AutoModeloForCausalLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ccgt8Elpzoic",
   "metadata": {
    "id": "Ccgt8Elpzoic"
   },
   "source": [
    "##Antes de Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jpj0k9md4HPz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jpj0k9md4HPz",
    "outputId": "37f84742-c2af-4628-c34e-63b1a5aa87d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estopa es una banda espa침ola conocida por su estilo hard rock y su sonido punk. Formada en 1982 en Oviedo por Estel Mena y Juan Pablo Estel. Entre 1987 y 1993 estuvo integrada por Estel Mena, el bajista de la banda, que pasar칤a luego a formar la banda Man치.\n",
      "\n",
      "Historia \n",
      "\n",
      "Estol칩 fue\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Dataset con texto continuo sobre Estopa\n",
    "datos = [\n",
    "    {\"text\": \"Estopa es un d칰o musical espa침ol formado por los hermanos David y Jos칠 Manuel Mu침oz. La banda se fund칩 en 1999 en Cornell de Llobregat, Barcelona. Su estilo musical combina flamenco, rock y rumba catalana, creando un sonido 칰nico que los ha llevado a la fama.\"},\n",
    "    {\"text\": \"El primer 치lbum de Estopa, titulado 'Estopa', se lanz칩 en  1999 y fue un 칠xito inmediato, vendiendo m치s de un mill칩n de copias. Canciones como 'La raja de tu falda' y 'Como Camar칩n' se convirtieron en cl치sicos.\"},\n",
    "    {\"text\": \"A lo largo de su carrera, Estopa ha lanzado m치s de 10 discos de estudio, manteniendo su caracter칤stico estilo y evolucionando con nuevas influencias. Su 치lbum 'Destrangis' consolid칩 a칰n m치s su 칠xito con canciones como 'Vino tinto'.\"},\n",
    "    {\"text\": \"Estopa ha ganado numerosos premios, incluidos los Premios Ondas y los 40 Principales, que reconocen su contribuci칩n a la m칰sica espa침ola. Sus conciertos son conocidos por su energ칤a y conexi칩n con el p칰blico.\"},\n",
    "    {\"text\": \"La ciudad natal de los hermanos, Cornell de Llobregat, influy칩 profundamente en su m칰sica. La mezcla cultural y las tradiciones flamencas del lugar se reflejan en sus letras y melod칤as.\"},\n",
    "    {\"text\": \"Adem치s de su m칰sica, Estopa es conocido por sus letras llenas de humor y referencias cotidianas. Estas caracter칤sticas los han hecho destacar y conectar con una audiencia amplia y diversa.\"},\n",
    "    {\"text\": \"Estopa contin칰a siendo una de las bandas m치s queridas en Espa침a, manteniendo su esencia mientras exploran nuevas direcciones en su m칰sica. Su legado perdurar치 como un s칤mbolo de creatividad y autenticidad en la m칰sica espa침ola.\"}\n",
    "]\n",
    "\n",
    "# Crear un Dataset compatible con Hugging Face\n",
    "dataset = Dataset.from_list(datos)\n",
    "\n",
    "# Cargar el modelo y el tokenizador preentrenados en espa침ol\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "modelo = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizador = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Usar eos_token_id como pad_token_id\n",
    "tokenizador.pad_token = tokenizador.eos_token\n",
    "modelo.config.pad_token_id = tokenizador.eos_token_id\n",
    "\n",
    "# Tokenizar el dataset\n",
    "def procesar_datos(ejemplo):\n",
    "    tokenizado = tokenizador(\n",
    "    ejemplo[\"text\"],              # El texto de entrada que se va a tokenizar\n",
    "    max_length=128,               # M치xima longitud de la secuencia tokenizada\n",
    "    truncation=True,              # Recorta el texto si supera max_length\n",
    "    padding=\"max_length\",         # Rellena con tokens [PAD] hasta alcanzar max_length\n",
    "    return_tensors=\"pt\"           # Devuelve tensores de PyTorch (tambi칠n puede ser \"tf\" o \"np\")\n",
    ")\n",
    "\n",
    "    return {key: tensor.squeeze() for key, tensor in tokenizado.items()}\n",
    "\n",
    "dataset_procesado = dataset.map(procesar_datos)\n",
    "\n",
    "# Usar DataCollatorForLanguageModeling (para gestionar el padding correctamente)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizador, mlm=False  # mlm=False porque es modelado causal, no enmascarado\n",
    ")\n",
    "\n",
    "# Configuraci칩n del entrenamiento\n",
    "argumentos = TrainingArguments(\n",
    "    output_dir=\"./resultados\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"  # Desactiva W&B\n",
    ")\n",
    "\n",
    "# Ajustar las dimensiones del modelo al tokenizador (porque hemos a침adido un nuevo token, el de padding)\n",
    "modelo.resize_token_embeddings(len(tokenizador))\n",
    "\n",
    "# Crear el Trainer\n",
    "trainer = Trainer(\n",
    "    model=modelo,\n",
    "    args=argumentos,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_procesado,\n",
    "    tokenizer=tokenizador,\n",
    "    eval_dataset=dataset_procesado\n",
    ")\n",
    "\n",
    "def generar_texto(prompt, modelo, tokenizador, max_length=100):\n",
    "    \"\"\"\n",
    "    Funci칩n para generar texto con el modelo actual.\n",
    "    \"\"\"\n",
    "    inputs = tokenizador(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    modelo.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = modelo.generate(**inputs, max_length=max_length, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "    return tokenizador.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Prueba antes del ajuste fino\n",
    "prompt_test = \"Estopa es una banda espa침ola conocida por\"\n",
    "print(\"\\n游댳 **Generaci칩n de texto ANTES del ajuste fino:**\")\n",
    "print(generar_texto(prompt_test, modelo, tokenizador))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "--bgFz-w5NII",
   "metadata": {
    "id": "--bgFz-w5NII"
   },
   "source": [
    "##Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kFIdsAsI5Mr5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "kFIdsAsI5Mr5",
    "outputId": "a49496af-7150-4082-e358-82d22b099df9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.699319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.288860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.191700</td>\n",
       "      <td>0.172364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=1.026526893178622, metrics={'train_runtime': 42.1278, 'train_samples_per_second': 0.498, 'train_steps_per_second': 0.285, 'total_flos': 16684615729152.0, 'train_loss': 1.026526893178622, 'epoch': 3.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-S03Tn0m5H_w",
   "metadata": {
    "id": "-S03Tn0m5H_w"
   },
   "source": [
    "##Despu칠s del Ajuste fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ltc-htSH5HMH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltc-htSH5HMH",
    "outputId": "c8760ead-6013-4137-b8e5-941e8f6521f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **Generaci칩n de texto DESPU칄S del ajuste fino:**\n",
      "Estopa es una banda espa침ola conocida por sus letras llenas de humor y referencias cotidianas. Comenzaron a sonar en los a침os 90, siendo un de los grupos m치s queridos y queridos en Espa침a. Su legado perdurar치 como un s칤mbolo de creatividad y autenticidad en la m칰sica espa침ola. Su 치lbum 'Qu칠 bueno' se llev칩 a una raja de 칠xitos in\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n **Generaci칩n de texto DESPU칄S del ajuste fino:**\")\n",
    "print(generar_texto(prompt_test, modelo, tokenizador))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131hap-eWVSn",
   "metadata": {
    "id": "131hap-eWVSn"
   },
   "source": [
    "#2. Clasificaci칩n de Texto (AutoModelForSequenceClassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lsqvzYVWg4UQ",
   "metadata": {
    "id": "lsqvzYVWg4UQ"
   },
   "source": [
    "##Antes de ajuste fino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kiQUHPDcg3Hf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiQUHPDcg3Hf",
    "outputId": "907b76d9-e806-4dbc-f189-129af5eb691c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25\n",
      "Reporte de clasificaci칩n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.33      0.50      0.40         2\n",
      "    Positive       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.25         4\n",
      "   macro avg       0.17      0.25      0.20         4\n",
      "weighted avg       0.17      0.25      0.20         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "\n",
    "# Tokenizador y modelo preentrenado\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Positivo y Negativo\n",
    "\n",
    "#EVALUACI칍N\n",
    "texts = [\"Amazing movie!\", \"Terrible plot.\", \"Loved the characters!\", \"Not my taste.\"]\n",
    "true_labels = [1, 0, 1, 0]  # Etiquetas reales\n",
    "\n",
    "# Predicciones\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "predicted_classes = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "# Calcular m칠tricas\n",
    "accuracy = accuracy_score(true_labels, predicted_classes)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Reporte de clasificaci칩n:\")\n",
    "print(classification_report(true_labels, predicted_classes, target_names=[\"Negative\", \"Positive\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3qs340G_u95D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qs340G_u95D",
    "outputId": "77b43d75-63d1-4a5e-8134-5bc66000c776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASES PREDICHAS: [0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"CLASES PREDICHAS:\", predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O9C8pyXAh-TN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9C8pyXAh-TN",
    "outputId": "c2088129-a878-4ce6-bb0a-0203dc4bad74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: This movie is amazing, I loved it!\n",
      "Predicci칩n: Negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Texto de prueba\n",
    "text = \"This movie is amazing, I loved it!\"\n",
    "\n",
    "# Tokenizaci칩n y predicci칩n\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Mover las entradas al mismo dispositivo que el modelo (GPU si est치 disponible)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # predicciones sin normalizar, es decir, los valores antes de aplicar una funci칩n como softmax.\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Mostrar resultado\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}  # Cambia seg칰n las etiquetas de tu modelo\n",
    "print(f\"Texto: {text}\")\n",
    "print(f\"Predicci칩n: {label_map[predicted_class]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Rpo7D2TiAI4",
   "metadata": {
    "id": "0Rpo7D2TiAI4"
   },
   "source": [
    "##AJUSTE FINO CON DATASET DE IMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "msAxe2GcTwTe",
   "metadata": {
    "id": "msAxe2GcTwTe"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el dataset IMDb (an치lisis de sentimientos)\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "# Tokenizador y modelo preentrenado\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Positivo y Negativo\n",
    "\n",
    "# Preprocesar datos\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Usar DataCollator para hacer padding din치mico\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Configuraci칩n del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Configurar el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000)),\n",
    "    eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dTANfgKfwbtB",
   "metadata": {
    "id": "dTANfgKfwbtB"
   },
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z8sY6LyPXIwz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8sY6LyPXIwz",
    "outputId": "566e9b0a-00c6-46fb-8e7c-3b77fd903a0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: This movie is amazing, I loved it!\n",
      "Predicci칩n: Positive\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Texto de prueba\n",
    "text = \"This movie is amazing, I loved it!\"\n",
    "\n",
    "# Tokenizaci칩n y predicci칩n\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Mover las entradas al mismo dispositivo que el modelo (GPU si est치 disponible)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # predicciones sin normalizar, es decir, los valores antes de aplicar una funci칩n como softmax.\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Mostrar resultado\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}  # Cambia seg칰n las etiquetas de tu modelo\n",
    "print(f\"Texto: {text}\")\n",
    "print(f\"Predicci칩n: {label_map[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8tnEDi-eetQ4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tnEDi-eetQ4",
    "outputId": "c9f47763-c68f-458a-8963-d2d0c4ae30a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Reporte de clasificaci칩n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00         2\n",
      "    Positive       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#EVALUACI칍N\n",
    "texts = [\"Amazing movie!\", \"Terrible plot.\", \"Loved the characters!\", \"Not my taste.\"]\n",
    "true_labels = [1, 0, 1, 0]  # Etiquetas reales\n",
    "\n",
    "# Predicciones\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "predicted_classes = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "# Calcular m칠tricas\n",
    "accuracy = accuracy_score(true_labels, predicted_classes)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Reporte de clasificaci칩n:\")\n",
    "print(classification_report(true_labels, predicted_classes, target_names=[\"Negative\", \"Positive\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2xjuhQLgV-qm",
   "metadata": {
    "id": "2xjuhQLgV-qm"
   },
   "source": [
    "#2. Reconocimiento de Entidades Nombradas (AutoModelForTokenClassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6YF96ZpjzbY7",
   "metadata": {
    "id": "6YF96ZpjzbY7"
   },
   "source": [
    "##Antes de Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UM0CR5M4TwTf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "a2a411cecd2945648a242e31ac01c3f4",
      "0360a3748c384c03a8c0ed6f4f4c4dc0",
      "35fab11e06734a66b5ee1fceda2a97ab",
      "9cb0ad0703d94e2b90eb7380267d629a",
      "ac6bdcf263f14eaaa76bc0ad97170018",
      "28e324dea91e4d40b148619669b70361",
      "8674e8e77faf473c864b141efd2549e2",
      "6a51823a124c4ff19d73640a18c40f19",
      "21621fd2a943401fbd4be8df5d4c5ac5",
      "73006f85805647bd87f6e5b61c906305",
      "891bb4789ae24bf0b7da3b360ffccf2d"
     ]
    },
    "id": "UM0CR5M4TwTf",
    "outputId": "e2b0c78f-533d-4d45-dbe0-3716aa43382a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a411cecd2945648a242e31ac01c3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'My', 'name', 'is', 'Wolfgang', 'and', 'I', 'live', 'in', 'Berlin', '[SEP]']\n",
      "Etiquetas predichas: ['I-ORG', 'I-ORG', 'I-MISC', 'I-ORG', 'I-MISC', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-MISC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-820689317>:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el dataset CoNLL-2003\n",
    "#-- El dataset CoNLL-2003 es un conjunto de datos muy utilizado para tareas de Reconocimiento de Entidades Nombradas (Named Entity Recognition, NER) en Procesamiento del Lenguaje Natural.\n",
    "#-- Fue introducido en la conferencia CoNLL-2003 shared task, organizada por la conferencia de la Association for Computational Linguistics (ACL).\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=\"True\")\n",
    "\n",
    "# Tokenizador y modelo preentrenado\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(dataset[\"train\"].features[\"ner_tags\"].feature.names))\n",
    "\n",
    "# Preprocesar datos\n",
    "def preprocess_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = [-100 if word_id is None else label[word_id] for word_id in word_ids]\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Usar DataCollator para alineaci칩n de etiquetas\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Configuraci칩n del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Configurar el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000)),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(500)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Texto de prueba\n",
    "text = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "# Tokenizaci칩n y predicci칩n\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "# Obtener las etiquetas predichas\n",
    "predicted_labels = [dataset[\"train\"].features[\"ner_tags\"].feature.names[p] for p in predictions[0].tolist()]\n",
    "\n",
    "# Imprimir los tokens y sus etiquetas predichas\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "print(\"Etiquetas predichas:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4S5xS9AcD6zT",
   "metadata": {
    "id": "4S5xS9AcD6zT"
   },
   "source": [
    "##Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81rtQrOED2ST",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "81rtQrOED2ST",
    "outputId": "b5052c81-7fb6-435e-ad26-df1276f98644"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.149108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.124064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.100066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.16891073608398438, metrics={'train_runtime': 29.0765, 'train_samples_per_second': 206.352, 'train_steps_per_second': 12.897, 'total_flos': 152435476445472.0, 'train_loss': 0.16891073608398438, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g40vv2BRD8Ef",
   "metadata": {
    "id": "g40vv2BRD8Ef"
   },
   "source": [
    "##Despu칠s de Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "RYv32iG7WYUE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYv32iG7WYUE",
    "outputId": "3c8b9f1c-98b3-43a8-90af-10863293582a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'My', 'name', 'is', 'Wolfgang', 'and', 'I', 'live', 'in', 'Berlin', '[SEP]']\n",
      "Etiquetas predichas: ['O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Texto de prueba\n",
    "text = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "# Tokenizaci칩n y predicci칩n\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "# Obtener las etiquetas predichas\n",
    "predicted_labels = [dataset[\"train\"].features[\"ner_tags\"].feature.names[p] for p in predictions[0].tolist()]\n",
    "\n",
    "# Imprimir los tokens y sus etiquetas predichas\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "print(\"Etiquetas predichas:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9N1xCWdIWYpn",
   "metadata": {
    "id": "9N1xCWdIWYpn"
   },
   "source": [
    "#3. Preguntas y Respuestas (AutoModelForQuestionAnswering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MO_N7fboLKJ1",
   "metadata": {
    "id": "MO_N7fboLKJ1"
   },
   "source": [
    "##Antes de Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "K5nSUvn1TwTg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5nSUvn1TwTg",
    "outputId": "4611a0e6-4cbf-4503-d5ed-4b96dd350a2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: Where is Badajoz placed?\n",
      "Contexto: Badajoz is a city located in the autonomous community of Extremadura, in southwestern Spain, near the Portuguese border.\n",
      "It is the largest city in the region by population and surface area. Founded during the Moorish period, Badajoz preserves\n",
      "important historical remains, such as its massive Alcazaba, one of the largest Arab citadels in Europe. The city's architecture\n",
      "also reflects Gothic and Renaissance influences, visible in its cathedral and old quarters. Badajoz hosts several cultural events,\n",
      "including the Carnival of Badajoz, considered one of the most important in Spain. It is also an academic and economic center,\n",
      "home to one of the campuses of the University of Extremadura. The city has a hot-summer Mediterranean climate, with mild winters\n",
      "and very hot, dry summers. The local economy relies on services, trade, agriculture, and cross-border cooperation with Portugal,\n",
      "particularly with the nearby city of Elvas. Extremadura as a region is known for its natural parks, traditional cuisine, and\n",
      "products such as Iberian ham and paprika from La Vera.\n",
      "Respuesta: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-57-2297793452>:81: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el dataset SQuAD v2\n",
    "#--El dataset SQuAD v2 (Stanford Question Answering Dataset, versi칩n 2.0) es un conjunto de datos muy influyente en el campo del Question Answering (QA), espec칤ficamente para tareas de pregunta-respuesta extractiva sobre texto.\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Tokenizador y modelo preentrenado\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Preprocesar datos\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=True\n",
    "    )\n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = inputs[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answer = answers[sample_index]\n",
    "        if len(answer[\"answer_start\"]) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answer[\"text\"][0])\n",
    "            token_start_index = 0\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "            if offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_positions.append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_positions.append(token_end_index + 1)\n",
    "            else:\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Usar DefaultDataCollator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest', max_length=512) #max_lenght para evitar error de memoria\n",
    "# Configuraci칩n del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Configurar el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(6000)),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(750)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Ejemplo de pregunta y contexto\n",
    "context = \"\"\"Badajoz is a city located in the autonomous community of Extremadura, in southwestern Spain, near the Portuguese border.\n",
    "It is the largest city in the region by population and surface area. Founded during the Moorish period, Badajoz preserves\n",
    "important historical remains, such as its massive Alcazaba, one of the largest Arab citadels in Europe. The city's architecture\n",
    "also reflects Gothic and Renaissance influences, visible in its cathedral and old quarters. Badajoz hosts several cultural events,\n",
    "including the Carnival of Badajoz, considered one of the most important in Spain. It is also an academic and economic center,\n",
    "home to one of the campuses of the University of Extremadura. The city has a hot-summer Mediterranean climate, with mild winters\n",
    "and very hot, dry summers. The local economy relies on services, trade, agriculture, and cross-border cooperation with Portugal,\n",
    "particularly with the nearby city of Elvas. Extremadura as a region is known for its natural parks, traditional cuisine, and\n",
    "products such as Iberian ham and paprika from La Vera.\"\"\"\n",
    "question = \"Where is Badajoz placed?\"\n",
    "# Tokenizar la pregunta y el contexto\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Realizar la inferencia\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtener las respuestas\n",
    "answer_start_index = torch.argmax(outputs.start_logits)\n",
    "answer_end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "\n",
    "predict_answer_tokens = inputs[\"input_ids\"][0, answer_start_index : answer_end_index + 1]\n",
    "answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(f\"Pregunta: {question}\")\n",
    "print(f\"Contexto: {context}\")\n",
    "print(f\"Respuesta: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tWQIYTcgEyD0",
   "metadata": {
    "id": "tWQIYTcgEyD0"
   },
   "source": [
    "##Entrenar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6KrNRIHNEw8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "6KrNRIHNEw8a",
    "outputId": "902e7788-67fd-4158-b668-8f87c7cf061c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 04:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.890600</td>\n",
       "      <td>1.882181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.629400</td>\n",
       "      <td>1.981344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.070600</td>\n",
       "      <td>1.998787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.612900</td>\n",
       "      <td>2.429250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>2.744407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=1.2711611165364582, metrics={'train_runtime': 277.9042, 'train_samples_per_second': 107.951, 'train_steps_per_second': 13.494, 'total_flos': 2939694750720000.0, 'train_loss': 1.2711611165364582, 'epoch': 5.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H-CjfV4oEzeq",
   "metadata": {
    "id": "H-CjfV4oEzeq"
   },
   "source": [
    "##Despu칠s de Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "QaLjPYzbXRsu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QaLjPYzbXRsu",
    "outputId": "8e36a39a-89d4-4b03-edcf-31ba18169cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: where is Badajoz located?\n",
      "Contexto: Badajoz is a city located in the autonomous community of Extremadura, in southwestern Spain, near the Portuguese border.\n",
      "It is the largest city in the region by population and surface area. Founded during the Moorish period, Badajoz preserves\n",
      "important historical remains, such as its massive Alcazaba, one of the largest Arab citadels in Europe. The city's architecture\n",
      "also reflects Gothic and Renaissance influences, visible in its cathedral and old quarters. Badajoz hosts several cultural events,\n",
      "including the Carnival of Badajoz, considered one of the most important in Spain. It is also an academic and economic center,\n",
      "home to one of the campuses of the University of Extremadura. \n",
      "The city has a hot-summer Mediterranean climate, with mild winters\n",
      "and very hot, dry summers. The local economy relies on services, trade, agriculture, and cross-border cooperation with Portugal,\n",
      "particularly with the nearby city of Elvas. \n",
      "Extremadura as a region is known for its natural parks, traditional cuisine, and\n",
      "products such as Iberian ham and paprika from La Vera.\n",
      "Respuesta: [CLS] where is badajoz located? [SEP] badajoz is a city located in the autonomous community of extremadura, in southwestern spain\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de pregunta y contexto\n",
    "context = \"\"\"Badajoz is a city located in the autonomous community of Extremadura, in southwestern Spain, near the Portuguese border.\n",
    "It is the largest city in the region by population and surface area. Founded during the Moorish period, Badajoz preserves\n",
    "important historical remains, such as its massive Alcazaba, one of the largest Arab citadels in Europe. The city's architecture\n",
    "also reflects Gothic and Renaissance influences, visible in its cathedral and old quarters. Badajoz hosts several cultural events,\n",
    "including the Carnival of Badajoz, considered one of the most important in Spain. It is also an academic and economic center,\n",
    "home to one of the campuses of the University of Extremadura.\n",
    "The city has a hot-summer Mediterranean climate, with mild winters\n",
    "and very hot, dry summers. The local economy relies on services, trade, agriculture, and cross-border cooperation with Portugal,\n",
    "particularly with the nearby city of Elvas.\n",
    "Extremadura as a region is known for its natural parks, traditional cuisine, and\n",
    "products such as Iberian ham and paprika from La Vera.\"\"\"\n",
    "question = \"where is Badajoz located?\"\n",
    "# Tokenizar la pregunta y el contexto\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Realizar la inferencia\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtener las respuestas\n",
    "answer_start_index = torch.argmax(outputs.start_logits)\n",
    "answer_end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "predict_answer_tokens = inputs[\"input_ids\"][0, answer_start_index : answer_end_index + 1]\n",
    "answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(f\"Pregunta: {question}\")\n",
    "print(f\"Contexto: {context}\")\n",
    "print(f\"Respuesta: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ZtfbyLRA88Mo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtfbyLRA88Mo",
    "outputId": "572caf79-9a3e-4f1b-82c9-851177fa90cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: which is the most important cultural event in Badajoz? \n",
      "Respuesta: carnival\n"
     ]
    }
   ],
   "source": [
    "question = \"which is the most important cultural event in Badajoz? \"\n",
    "# Tokenizar la pregunta y el contexto\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Realizar la inferencia\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtener las respuestas\n",
    "answer_start_index = torch.argmax(outputs.start_logits)\n",
    "answer_end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "predict_answer_tokens = inputs[\"input_ids\"][0, answer_start_index : answer_end_index + 1]\n",
    "answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(f\"Pregunta: {question}\")\n",
    "print(f\"Respuesta: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn05gQ2GZJsj",
   "metadata": {
    "id": "nn05gQ2GZJsj"
   },
   "source": [
    "#AutoModelForMaskedLM.from_pretrained (Masked Language Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5K-TXCF77wjf",
   "metadata": {
    "id": "5K-TXCF77wjf"
   },
   "outputs": [],
   "source": [
    "pip install wikipedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C67aWuCaKDFM",
   "metadata": {
    "id": "C67aWuCaKDFM"
   },
   "source": [
    "##Antes de ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "Uw-LKhTLlMh7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uw-LKhTLlMh7",
    "outputId": "90eadba0-4137-4334-bef7-cfffa6cc00c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del ajuste fino:\n",
      "Texto: The capital city of Extremadura is [MASK].\n",
      "Predicci칩n: madrid\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-3129552404>:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import wikipedia\n",
    "\n",
    "wikipedia.set_lang(\"en\")\n",
    "# Dataset para Masked LM\n",
    "extremadura_text = wikipedia.page(\"Extremadura\").content\n",
    "ds = Dataset.from_dict({\"text\": [extremadura_text]*1000})\n",
    "\n",
    "\n",
    "# Tokenizador y modelo\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Tokenizaci칩n\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "\n",
    "# DataCollator para Masked LM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.5)\n",
    "\n",
    "# Configuraci칩n de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Comprobar rendimiento antes del ajuste fino\n",
    "text = \"The capital city of Extremadura is [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predicted_token_id = outputs.logits[0, inputs.input_ids[0].tolist().index(tokenizer.mask_token_id)].argmax().item()\n",
    "print(\"Antes del ajuste fino:\")\n",
    "print(f\"Texto: {text}\")\n",
    "print(f\"Predicci칩n: {tokenizer.decode(predicted_token_id)}\\n\")\n",
    "\n",
    "# Entrenamiento\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0q1I7VnEzPhq",
   "metadata": {
    "id": "0q1I7VnEzPhq"
   },
   "source": [
    "##Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "uyeT216FyFb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "uyeT216FyFb2",
    "outputId": "e6d91526-55e6-46b4-aa41-879f12335c9b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 00:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.013320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.005356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=315, training_loss=0.10081304519895523, metrics={'train_runtime': 58.7365, 'train_samples_per_second': 85.126, 'train_steps_per_second': 5.363, 'total_flos': 329006016000000.0, 'train_loss': 0.10081304519895523, 'epoch': 5.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uCgv1IGYKFbA",
   "metadata": {
    "id": "uCgv1IGYKFbA"
   },
   "source": [
    "##Despu칠s del Ajuste fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2WTdS4g4SnOX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2WTdS4g4SnOX",
    "outputId": "c17de504-ae86-426f-9877-09412eb5cdc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despu칠s del ajuste fino:\n",
      "Texto: The capital city of Extremadura is [MASK].\n",
      "Predicci칩n: merida\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Comprobar rendimiento despu칠s del ajuste fino\n",
    "outputs = model(**inputs)\n",
    "predicted_token_id = outputs.logits[0, inputs['input_ids'][0].tolist().index(tokenizer.mask_token_id)].argmax().item()\n",
    "print(\"Despu칠s del ajuste fino:\")\n",
    "print(f\"Texto: {text}\")\n",
    "print(f\"Predicci칩n: {tokenizer.decode(predicted_token_id)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k8ZdxG-wZQXN",
   "metadata": {
    "id": "k8ZdxG-wZQXN"
   },
   "source": [
    "#3. AutoModelForMultipleChoice (Selecci칩n M칰ltiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10HqzvvYKIbT",
   "metadata": {
    "id": "10HqzvvYKIbT"
   },
   "source": [
    "##Antes del ajuste fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "qcKZRxIeIrGv",
   "metadata": {
    "id": "qcKZRxIeIrGv"
   },
   "outputs": [],
   "source": [
    "#borro todas la variables\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3Yvwl53WJ9Of",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Yvwl53WJ9Of",
    "outputId": "12abd488-c7a7-47a3-d6ff-bd1b4c61e01c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-107-1996011700>:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del ajuste fino\n",
      "Logits: tensor([[-0.1461, -0.1701,  0.0089, -0.1836]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Opci칩n elegida: She called her teacher to ask about homework.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# 1. Cargar el dataset PIQA\n",
    "dataset = load_dataset(\"piqa\")\n",
    "\n",
    "# 2. Usar un modelo base\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
    "\n",
    "# 3. Preprocesado: convertir en formato multiple-choice\n",
    "def preprocess_function(examples):\n",
    "    # Cada ejemplo tiene un 'goal' y dos opciones: 'sol1' y 'sol2'\n",
    "    first_sentences = []\n",
    "    second_sentences = []\n",
    "    for goal, sol1, sol2 in zip(examples[\"goal\"], examples[\"sol1\"], examples[\"sol2\"]):\n",
    "        first_sentences.extend([goal, goal])\n",
    "        second_sentences.extend([sol1, sol2])\n",
    "\n",
    "    # Tokenize the questions and choices\n",
    "    tokenized = tokenizer(first_sentences, second_sentences, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # Agrupar en pares (batch_size, num_choices, seq_len)\n",
    "    result = {\n",
    "        k: [tokenized[k][i:i+2] for i in range(0, len(tokenized[k]), 2)]\n",
    "        for k in tokenized\n",
    "    }\n",
    "    result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "# Aplicamos el preprocesado a train y validation\n",
    "encoded_train = dataset[\"train\"].select(range(2000)).map(preprocess_function, batched=True)\n",
    "encoded_valid = dataset[\"validation\"].select(range(500)).map(preprocess_function, batched=True)\n",
    "\n",
    "# DatasetWrapper para que funcione con Trainer\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: bool = True\n",
    "    max_length: int = None\n",
    "\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "\n",
    "        flattened_features = []\n",
    "        for feature in features:\n",
    "            for i in range(num_choices):\n",
    "                flattened_features.append(\n",
    "                    {k: feature[k][i] for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] if k in feature}\n",
    "                )\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        batch = {\n",
    "            k: v.view(batch_size, num_choices, -1) for k, v in batch.items()\n",
    "        }\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# 4. Argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./piqa_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Comprobar logits antes del entrenamiento\n",
    "prompt = [\"Laura went to the kitchen and opened the fridge. She saw that the milk was expired.\"]\n",
    "choices = [\n",
    "    \"She threw it away and took a juice instead.\",         # coherente\n",
    "    \"She painted the fridge green.\",                       # absurda\n",
    "    \"She called her teacher to ask about homework.\",       # irrelevante\n",
    "    \"She put the expired milk in her backpack.\"            # extra침o e incorrecto\n",
    "]\n",
    "inputs = tokenizer(prompt * len(choices), choices, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "# Re-dimensionamos para que sea (batch_size=1, num_choices=4, seq_length)\n",
    "for k in inputs:\n",
    "    inputs[k] = inputs[k].unsqueeze(0)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
    "best_choice = choices[predicted_idx]\n",
    "print(\"Antes del ajuste fino\")\n",
    "print(f\"Logits: {outputs.logits}\\n\")\n",
    "print(\"Opci칩n elegida:\", best_choice)\n",
    "\n",
    "# Entrenamos con Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XsWVz1XmzBC_",
   "metadata": {
    "id": "XsWVz1XmzBC_"
   },
   "source": [
    "##Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3ro2PJRvy-m_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "3ro2PJRvy-m_",
    "outputId": "a20406b9-8277-426e-aa88-efdc871ce564"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 02:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.683393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.634900</td>\n",
       "      <td>0.704613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.468100</td>\n",
       "      <td>0.736609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.771213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.5414263687133789, metrics={'train_runtime': 153.0087, 'train_samples_per_second': 65.356, 'train_steps_per_second': 13.071, 'total_flos': 1315543464960000.0, 'train_loss': 0.5414263687133789, 'epoch': 5.0})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O451FIqBKN4-",
   "metadata": {
    "id": "O451FIqBKN4-"
   },
   "source": [
    "##Despu칠s del ajuste fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "BpLdTpqiAKLB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BpLdTpqiAKLB",
    "outputId": "a0b4455c-efa1-4787-9ba5-eed3a68034fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despu칠s del ajuste fino:\n",
      "Logits: tensor([[ 0.8619,  0.1487, -2.7114,  0.3739]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Opci칩n elegida: She threw it away and took a juice instead.\n"
     ]
    }
   ],
   "source": [
    "# Comprobar rendimiento despu칠s del ajuste fino\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
    "best_choice = choices[predicted_idx]\n",
    "print(\"Despu칠s del ajuste fino:\")\n",
    "print(f\"Logits: {outputs.logits}\\n\")\n",
    "print(\"Opci칩n elegida:\", best_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9spSksROZYR4",
   "metadata": {
    "id": "9spSksROZYR4"
   },
   "source": [
    "#4. AutoModelForSeq2SeqLM.from_pretrained (Traducci칩n/Secuencia a Secuencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HGgrBd6QKSdv",
   "metadata": {
    "id": "HGgrBd6QKSdv"
   },
   "source": [
    "##Antes del ajuste fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "lZipDlMmllkg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476,
     "referenced_widgets": [
      "b465ce49c3664831b75bf5da55f07924",
      "17ebe2cc3d964c6ba3fbc7a599d4298f",
      "5d941b9a82854cbaa4a92541cbe02423",
      "977195537bfa4fdb9a3b29bafc78c754",
      "b0dc87c97c3d46c18dc59f58e5369f47",
      "d29b3ae9a3834a6097efabbf045c92bf",
      "20e4404437034b6ab81413f8e40982f8",
      "c59de29c0b0b49ee83541c9616b401ba",
      "4b6415b7d321471bbdf0feb658e11629",
      "c228c6a5cb3e4fb4a8dc1bcab3a1beb8",
      "b93cd88dd42a41e19973dcd184f19be1",
      "882305a2acdc41ebbdb02cd7ba97e82a",
      "60e1e992884b411c9be2a20e16b60999",
      "a5d00cdefcea40cea760ae18e267d93b",
      "2d4ef9cc146a4f4ab06db159f78544dd",
      "57805feb85c34f559aaf17b1c0228e8b",
      "5e30ede5b64343d09cd8a77e1d5b9719",
      "5d9d42f2e5af465e873278cb8ef4d432",
      "eee67b80f4e54fbb9d11888e6114e4b1",
      "2c1c4f1ecadc469c8bc7e70e62f01f93",
      "031eb224e9094aacaadeef7889e2cd42",
      "189710cfb3d54af4a5b9a84487b11956",
      "596914882da040ccade4334b12d23303",
      "8570643f55a148f4ae39009b37a9b203",
      "29a519abcb8a4203b694a162cd6d40a5",
      "a71f81f3b6c24ccfb922bbb3ab3de032",
      "6f009143b83147ad848c18b3c96a5d61",
      "036c456aa39243c2816a5fe4def6f584",
      "7a746c51cab744a08b0476797e955764",
      "639c023ca8274a55a176463b697aae25",
      "5ae94f90881c4dc59d82aca4e7d2a9f4",
      "c4393a9c8c9c44d4b7f8fe3aa38e0f71",
      "77338560b6794bf28abf3a59385270b1",
      "cb53c61a0b8943ad8271f72b025dfe15",
      "851eb5061a714b74bc26d971d7a9bc0e",
      "acb621536953429fafd5981344e1b88f",
      "ee85f5ec3f444cc29294670d551fb7d4",
      "29347b55e2d846eead938ec17b8007e8",
      "ffac99be67964628932cc2dbdc7b014f",
      "5b70438893d64960adc849fd546f2f09",
      "f0180735f7134e2f82e165fb71c0803a",
      "6d4d2b2af4fb42f0b3069151ad1b6d05",
      "33119475f2b248cbb027081a39e9748d",
      "f846946eeccd49829967f9de49f28f3d",
      "e79670a5848b4669a7de6252f4d11ee2",
      "19111d9c8898436d9ba9fa5d7b3ea5bb",
      "6633733c62c84cd2b6055a3cf930f7af",
      "09abb8cfcb1c438c900ae495cc8e8883",
      "e05ad0191d7d47168d6b630109c64097",
      "7b99f8bdc40c4294bbbd1079311fa78f",
      "14c79e7d2de0435288fe411a8617c502",
      "af32b5b1913c4321b5da99ae96526860",
      "fa7178ee2560448b82003754726272e1",
      "af5dc34b17b7420a906e364b536b1270",
      "290921cd68674ee581ee0007d95cfaac",
      "9c6cd1307f1d462d959999f4a2681f3a",
      "72582ff075604ecfbaa974b8f24fdf05",
      "9867a80d4890467692b9d0f12f9e8e7f",
      "7bc68e1d469e449ca9b1388945a04bd4",
      "071fa86dc1df40e2bfb0ca03114f9654",
      "37a5f3cb94ab4d52ba1c8941a098ffb9",
      "1b228cd94aed45e099ad5e701e12b7e7",
      "add082aa08a649d19756e6bca732bd1c",
      "b708d28b55e44f83b02e5b36bc601798",
      "be91e955c1024841ac8156f3f1310f7e",
      "3958f736693d4329afc403dfd0c907d7",
      "e3bd48e755d642828b9c55f92c688a44",
      "060a26074f7244169cbdc8c9915dbb2f",
      "623e9201a3144f0683baef2e0f5818c2",
      "aaca669ac042482796438dd0c436fb4e",
      "b029a17ab7b64c29a37978af546a5aec",
      "90bc5b3af896428f92145d073558e95a",
      "9f8d4d9a9650455e9b0185473177d38d",
      "51e27e58e66b41cf86934248d5538aaf",
      "ee671209a31f4489842b8b74875dcc78",
      "cd2c9511393b456f843c3a5aebe8893a",
      "cade2fbdf18c4e7d9ab39f3fa55c9edc",
      "030b68b68cb14e2bbafbebf52e67fd14",
      "bb9685455e8740a5abdcbf01313b2aee",
      "9e190f7b332a45ef9f96fe401bab35fc",
      "cbd52b0d33b94f13b487b00bc1b90f7d",
      "3d2f10106812441ea36f1cffafa1d043",
      "050f39739952421e95288781e929d061",
      "6e38673f3c5d45c8b084ad27d720f0ac",
      "dbd8167b296c4e9fa17c93a913f99d5e",
      "6ef4bdaf6548449085e0087407406894",
      "9d9c2f1a44c249ddaf3b1103366863cc",
      "2c554786d3184d5d85277a207bc2e9f2",
      "9c871495d6664509b49ef2525536ef1b",
      "866a8fd88c5e4d05a0ce24be7b62bc0f",
      "21e13b6c80f74ba7a344f843e6ee09df",
      "d0e4e3dfcbdc4891a733af3344d42df2",
      "346db81167d644a8a5f0fac641deea55",
      "7892b4a22dd2407f9cb3c19c141a47d2",
      "0b932a8a9ad942ebaf9507f4536ef4d6",
      "5a7816396f88425d9e56b85e049b71e4",
      "53eb6407971e4665b1a3771639bda791",
      "9a896c86d6dd48a7b69c09bbcc2dfde0",
      "d068299a5c124a65ab32a5065f0e4adc",
      "a9db2689f31740b1861c7946eca38b77",
      "1220e4a0f0534f67b2b41e2692f77e06",
      "3fc956422af74731b0fdbf8f3383fe11",
      "782d78534efd49198315d44ccf93db14",
      "684b56c12b2c4ba8a6e7e5878cbaaf8d",
      "bafec54e554e4f52926edf2a8304a337",
      "543d580a4ff248eebf82e48f275b8034",
      "dfb464a0046046f6bf7ce36d78113664",
      "0cafa7f4e77940e696fe2007e0c4d90a",
      "0da8ee5029344753a456e2e7e0299617",
      "92881e0c5f2d49cc8ad160e55f729da4",
      "b1783128c6f64a33b80dd1f9feacb70d",
      "c6b7de38ab88421b995ff807f06f7ab2",
      "0f93e1ba22744364bd30c36d72cb04b4",
      "963fb08ed79d4b5596282d54b188adec",
      "dcbff23628fb4ff68c3f8d7e9c6ea01a",
      "b940e46c89da486ab3b4608ff736750a",
      "1f6648b35f7d4c4591b2ca0575f2314e",
      "106e99f796ce4a98bd4a2cbae2a514cf",
      "76755af30bf24c22bd346343721b39eb",
      "d7b02dfd225c4346b9d1b6763d5da320",
      "c8822dd9d7c9441ca8432b69143fcfb3"
     ]
    },
    "id": "lZipDlMmllkg",
    "outputId": "ad146388-c55b-4c2c-db48-0be7c6990cc7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b465ce49c3664831b75bf5da55f07924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882305a2acdc41ebbdb02cd7ba97e82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/16.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596914882da040ccade4334b12d23303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/93470 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb53c61a0b8943ad8271f72b025dfe15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79670a5848b4669a7de6252f4d11ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6cd1307f1d462d959999f4a2681f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bd48e755d642828b9c55f92c688a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030b68b68cb14e2bbafbebf52e67fd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c871495d6664509b49ef2525536ef1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9db2689f31740b1861c7946eca38b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1783128c6f64a33b80dd1f9feacb70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del ajuste fino:\n",
      "Entrada: The book is on the table.\n",
      "Traducci칩n: Das Buch ist auf dem Tisch.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-61-1363106736>:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar dataset de traducci칩n\n",
    "dataset = load_dataset(\"opus_books\", \"en-es\")\n",
    "#Creamos split de validaci칩n (10% del train)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "dataset['validation'] = dataset['test']\n",
    "# Tokenizador y modelo\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Preprocesar datos para traducci칩n\n",
    "def preprocess_function(examples):\n",
    "    # \"translation\" es un diccionario con {\"en\": ..., \"es\": ...}\n",
    "    # Recuperamos la lista de textos en ingl칠s y espa침ol\n",
    "    en_texts = [item[\"en\"] for item in examples[\"translation\"]]\n",
    "    es_texts = [item[\"es\"] for item in examples[\"translation\"]]\n",
    "\n",
    "    # Construimos el prompt: \"translate English to Spanish: <texto_en>\"\n",
    "    inputs = [f\"translate English to Spanish: {text}\" for text in en_texts]\n",
    "    # El target ser치 directamente el texto en espa침ol\n",
    "    targets = [text for text in es_texts]\n",
    "\n",
    "    # Tokenizamos entradas y salidas\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    labels = tokenizer(targets, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # A침adimos las labels al diccionario de tokens\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Usar DataCollator para Seq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Configuraci칩n de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Comprobar rendimiento antes del ajuste fino\n",
    "def test_translation(model, tokenizer, input_text):\n",
    "    inputs = tokenizer(f\"translate English to Spanish: {input_text}\", return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(**inputs, max_length=50)\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "print(\"Antes del ajuste fino:\")\n",
    "input_text = \"The book is on the table.\"\n",
    "print(f\"Entrada: {input_text}\")\n",
    "print(f\"Traducci칩n: {test_translation(model, tokenizer, input_text)}\\n\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].shuffle(seed=42),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EKuYy_irzI3x",
   "metadata": {
    "id": "EKuYy_irzI3x"
   },
   "source": [
    "##Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "MyoH9BE1zMcY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "MyoH9BE1zMcY",
    "outputId": "6b9168ea-09c8-460a-be66-e7914b3c82d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15774' max='15774' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15774/15774 24:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.105300</td>\n",
       "      <td>0.994683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.052200</td>\n",
       "      <td>0.923904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.009000</td>\n",
       "      <td>0.905965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15774, training_loss=1.1174980832004753, metrics={'train_runtime': 1446.9772, 'train_samples_per_second': 174.411, 'train_steps_per_second': 10.901, 'total_flos': 8539018773921792.0, 'train_loss': 1.1174980832004753, 'epoch': 3.0})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9AqCp5c2KVvN",
   "metadata": {
    "id": "9AqCp5c2KVvN"
   },
   "source": [
    "##Despu칠s del Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "yIchufSxp8fj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIchufSxp8fj",
    "outputId": "ec51f072-78d8-4691-a49b-668781b6c0ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despu칠s del ajuste fino:\n",
      "Entrada: The book is on the table.\n",
      "Traducci칩n: El libro est치 en la mesa.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprobar rendimiento despu칠s del ajuste fino\n",
    "\n",
    "print(\"Despu칠s del ajuste fino:\")\n",
    "print(f\"Entrada: {input_text}\")\n",
    "print(f\"Traducci칩n: {test_translation(model, tokenizer, input_text)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
